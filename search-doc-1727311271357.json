{"searchDocs":[{"title":"xRx Demos","type":0,"sectionRef":"#","url":"/xrx-core/docs/demos/","content":"xRx Demos","keywords":"","version":"Next"},{"title":"Contributing to xRx","type":0,"sectionRef":"#","url":"/xrx-core/docs/contributing","content":"","keywords":"","version":"Next"},{"title":"How to Contribute​","type":1,"pageTitle":"Contributing to xRx","url":"/xrx-core/docs/contributing#how-to-contribute","content":" ","version":"Next","tagName":"h2"},{"title":"Reporting Issues​","type":1,"pageTitle":"Contributing to xRx","url":"/xrx-core/docs/contributing#reporting-issues","content":" If you encounter a bug or have a feature request, please create an issue on GitHub. When creating an issue, please include:  A clear and descriptive titleA detailed description of the issue or feature requestSteps to reproduce the issue (if applicable)Any relevant screenshots, logs, or code snippets  ","version":"Next","tagName":"h3"},{"title":"Contributing Code​","type":1,"pageTitle":"Contributing to xRx","url":"/xrx-core/docs/contributing#contributing-code","content":" We welcome contributions to the xRx framework and its sample applications. If you have any suggestions or improvements, please follow these steps:  Open a new issue on GitHub describing the proposed change or improvementFork the repositoryCreate a new branch for your featureCommit your changesPush to your branchCreate a pull request, referencing the issue you created  Note: Pull requests not backed by published issues may not be considered. This process ensures that all contributions are discussed and aligned with the project's goals before implementation.  ","version":"Next","tagName":"h3"},{"title":"Code Review Process​","type":1,"pageTitle":"Contributing to xRx","url":"/xrx-core/docs/contributing#code-review-process","content":" Your pull request will be reviewed by a project maintainerYou may be asked to make changes to your pull request based on feedbackOnce your pull request is approved, it will be merged into the main branch  ","version":"Next","tagName":"h3"},{"title":"Coding Standards​","type":1,"pageTitle":"Contributing to xRx","url":"/xrx-core/docs/contributing#coding-standards","content":" To maintain consistency across the project, please adhere to the following guidelines:  Follow the existing code style and formattingWrite clear, descriptive commit messagesInclude comments in your code where necessaryWrite and update tests for your changes  ","version":"Next","tagName":"h2"},{"title":"Community Guidelines​","type":1,"pageTitle":"Contributing to xRx","url":"/xrx-core/docs/contributing#community-guidelines","content":" We strive to maintain a welcoming and inclusive community. Please:  Be respectful and considerate in your interactions with other contributorsProvide constructive feedbackBe open to suggestions and improvements  ","version":"Next","tagName":"h2"},{"title":"Thank You!​","type":1,"pageTitle":"Contributing to xRx","url":"/xrx-core/docs/contributing#thank-you","content":" Your contributions play a vital role in the growth and improvement of xRx. We truly appreciate your efforts and look forward to your contributions! ","version":"Next","tagName":"h2"},{"title":"System Architecture","type":0,"sectionRef":"#","url":"/xrx-core/docs/how-it-works/system_architecture","content":"","keywords":"","version":"Next"},{"title":"Modular Components​","type":1,"pageTitle":"System Architecture","url":"/xrx-core/docs/how-it-works/system_architecture#modular-components","content":" The xRx system architecture consists of several components that interact with each other to provide build a reasoning based application. Below is a high-level overview of the system:    Client: Front end app experience which renders the UI and handles websocket communication with the Orchestrator. See directory hereOrchestrator: Manages the flow of data between various AI and traditional software components. See directory hereSTT (Speech-to-Text): Converts audio input to text. See directory hereTTS (Text-to-Speech): Converts text responses back to audio. See directory hereAgent: Responsible for the &quot;reasoning&quot; system of xRx. See directory hereGuardrails Proxy: A safety layer for the reasoning system. See directory here  ","version":"Next","tagName":"h2"},{"title":"Information Flow​","type":1,"pageTitle":"System Architecture","url":"/xrx-core/docs/how-it-works/system_architecture#information-flow","content":" These components then communicate via the following sequence diagram    ","version":"Next","tagName":"h2"},{"title":"Deployment Specifics​","type":1,"pageTitle":"System Architecture","url":"/xrx-core/docs/how-it-works/system_architecture#deployment-specifics","content":" xRx's deployment is designed to be modular in nature. This means that you can swap out any component of the system with your own custom implementation. The entire system is defined as a single docker-compose file with a single connected network. This allows for easy swapping of components and deployment to a variety of different environments.  A key design choice in xRx is the separation of the core framework from specific applications. The xRx core, which remains consistent across different applications, is imported as a submodule in each app. This core contains modules that are plug-and-play for custom applications, providing a foundation of reusable components.  This separation method allows for greater flexibility in application development. The xRx core includes containerized modules and reusable libraries:  Reusable libraries: These include the agent framework and UI library, which can be imported into each specific app.Containerized modules: The xRx system includes components such as TTS, STT, Guardrails, and the Orchestrator, which are defined as separate Docker containers. The docker-compose file located in the application folder starts each of these containerized components and connects them to the same network.  This modular structure, combined with the separation of core libraries and containerized components from the application-specific logic, enables developers to easily customize and extend xRx for their specific needs while benefiting from a solid, tested foundation. It also allows for easy swapping or upgrading of individual components without affecting the entire system. ","version":"Next","tagName":"h2"},{"title":"API Design","type":0,"sectionRef":"#","url":"/xrx-core/docs/how-it-works/api_design","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"API Design","url":"/xrx-core/docs/how-it-works/api_design#overview","content":" This document describes the communication flow between various components in the system, including the front end, orchestrator, reasoning agent, TTS (Text-to-Speech), and STT (Speech-to-Text) services. The orchestrator is the central component that manages WebSocket connections and coordinates communication between the front end, STT, TTS, and reasoning agent services. All of these communication flows can be visualized by the sequence diagram below.    ","version":"Next","tagName":"h2"},{"title":"Components​","type":1,"pageTitle":"API Design","url":"/xrx-core/docs/how-it-works/api_design#components","content":" The xRx system is designed with a modular architecture, separating the core framework from specific applications. This design allows for greater flexibility and customization in application development.  ","version":"Next","tagName":"h2"},{"title":"Application​","type":1,"pageTitle":"API Design","url":"/xrx-core/docs/how-it-works/api_design#application","content":" The application houses all the application-specific code. This structure ensures efficient development and maintenance of the application functionality.  Front End (Client)​ The front end is a Next.js client that interacts with the orchestrator via WebSocket connections. It sends user inputs (text or audio) and receives responses (text or audio) from the orchestrator. Reasoning Agent​ The reasoning agent processes user inputs and generates appropriate responses. It communicates with the orchestrator via HTTP requests which produce event streaming outputs via POST requests. The server is implemented using FastAPI in Python.  ","version":"Next","tagName":"h3"},{"title":"xRx Core​","type":1,"pageTitle":"API Design","url":"/xrx-core/docs/how-it-works/api_design#xrx-core","content":" The xRx Core contains the essential components that form the foundation of the xRx system. These components are designed to be reusable across different applications.  Orchestrator​ [Container] The orchestrator is a Fastify server that manages WebSocket connections with the front end and coordinates communication with the STT, TTS, and reasoning agent services. TTS (Text-to-Speech)​ [Container] The TTS service converts text responses from the reasoning agent into audio. It communicates with the orchestrator via WebSocket connections. The server is implemented using FastAPI in Python. STT (Speech-to-Text)​ [Container] The STT service converts audio inputs from the front end into text. It communicates with the orchestrator via WebSocket connections. The server is implemented using FastAPI in Python. React xRx Client Library​ [Library] A reusable React library that provides UI components and utilities for building the front-end of xRx-powered applications. It can be imported into each specific app to streamline development. Agent Framework​ [Library] A reusable library that provides the foundation for building reasoning agents. It includes common functionalities and structures that can be extended and customized for specific application needs.  These components are either containerized modules (like TTS, STT, Orchestrator) defined as separate Docker containers, or reusable libraries (like react-xrx-client and agent_framework) that can be imported into specific applications. This modular structure allows developers to easily customize and extend xRx for their specific needs while benefiting from a solid, tested foundation.  ","version":"Next","tagName":"h3"},{"title":"Communication Flow​","type":1,"pageTitle":"API Design","url":"/xrx-core/docs/how-it-works/api_design#communication-flow","content":" The communication flow between the components is designed to ensure seamless interaction and response generation. Below is a detailed breakdown of the communication steps and the data fields involved.  ","version":"Next","tagName":"h2"},{"title":"1. User Input​","type":1,"pageTitle":"API Design","url":"/xrx-core/docs/how-it-works/api_design#1-user-input","content":" When a user interacts with the front end, they can provide input in two forms: text or audio.  Text Input: The front end sends a text message to the orchestrator via WebSocket. Field: contentType: stringDescription: The text message input by the user. Audio Input: The front end sends an audio message to the orchestrator via WebSocket. Field: audioType: BufferDescription: The audio message input by the user.  ","version":"Next","tagName":"h3"},{"title":"2. Orchestrator Handling​","type":1,"pageTitle":"API Design","url":"/xrx-core/docs/how-it-works/api_design#2-orchestrator-handling","content":" Upon receiving the user input, the orchestrator processes it based on its type.  Text Input: The orchestrator appends the text message to the chat history and sends it to the reasoning agent.Audio Input: The orchestrator sends the audio message to the STT service for transcription. Field: audioType: BufferDescription: The audio message to be transcribed.  ","version":"Next","tagName":"h3"},{"title":"3. STT Service​","type":1,"pageTitle":"API Design","url":"/xrx-core/docs/how-it-works/api_design#3-stt-service","content":" The STT service is responsible for converting audio inputs into text.  The STT service transcribes the audio message into text and sends it back to the orchestrator via WebSocket. Field: transcriptionType: stringDescription: The text transcription of the audio message.  ","version":"Next","tagName":"h3"},{"title":"4. Reasoning Agent​","type":1,"pageTitle":"API Design","url":"/xrx-core/docs/how-it-works/api_design#4-reasoning-agent","content":" The orchestrator then forwards the processed input to the reasoning agent for generating a response.  The orchestrator sends the chat history (including the transcribed text) to the reasoning agent via an HTTP POST request. Conversational History Field: messagesType: ChatMessage[]Description: The chat history including the latest user input. Session Data Field: sessionType: { [key: string]: string }Description: Session-specific data to maintain context. Action Field: action (Optional)Type: objectDescription: Specific actions to be performed by the agent. These &quot;actions&quot; map to &quot;tools in the agent. The reasoning agent processes the input and generates a response, which it sends back to the orchestrator. Reasoning Agent Response Field: responseType: stringDescription: The response generated by the reasoning agent. Updated Session Data Field: sessionType: { [key: string]: string }Description: Updated session-specific data. For example (a new cart ID)  ","version":"Next","tagName":"h3"},{"title":"5. TTS Service​","type":1,"pageTitle":"API Design","url":"/xrx-core/docs/how-it-works/api_design#5-tts-service","content":" If the response from the reasoning agent is in text form and needs to be converted to audio, the orchestrator utilizes the TTS service.  The orchestrator sends the text to the TTS service via WebSocket. Field: textType: stringDescription: The text message to be converted to audio. The TTS service converts the text to audio and sends it back to the orchestrator via WebSocket. Field: audioType: BufferDescription: The audio version of the text message.  ","version":"Next","tagName":"h3"},{"title":"6. Response to Front End​","type":1,"pageTitle":"API Design","url":"/xrx-core/docs/how-it-works/api_design#6-response-to-front-end","content":" Finally, the orchestrator sends the response back to the front end, which can be in text or audio form.  Text Response: The orchestrator sends the text response from the reasoning agent to the front end via WebSocket. Field: contentType: stringDescription: The text response from the reasoning agent. Audio Response: The orchestrator sends the audio response from the TTS service to the front end via WebSocket. Field: audioType: BufferDescription: The audio response from the TTS service.  ","version":"Next","tagName":"h3"},{"title":"Environment Variables​","type":1,"pageTitle":"API Design","url":"/xrx-core/docs/how-it-works/api_design#environment-variables","content":" The orchestrator and other services use environment variables to configure hostnames, ports, and paths for communication. These variables are defined in a .env file. Each application's .env file contains all the necessary environment variables for that specific service.  ","version":"Next","tagName":"h2"},{"title":"Example Environment Variables​","type":1,"pageTitle":"API Design","url":"/xrx-core/docs/how-it-works/api_design#example-environment-variables","content":" STT_HOST=xrx-stt STT_PORT=8001 STT_PATH=/api/v1/ws TTS_HOST=xrx-tts TTS_PORT=8002 TTS_PATH=/api/v1/ws AGENT_HOST=xrx-reasoning AGENT_PORT=8003 AGENT_PATH=/run-reasoning-agent   ","version":"Next","tagName":"h3"},{"title":"Conclusion​","type":1,"pageTitle":"API Design","url":"/xrx-core/docs/how-it-works/api_design#conclusion","content":" This document provides an in-depth overview of how the various components in the system communicate with each other, including the specific data fields, their types, and why they are needed. The orchestrator plays a central role in managing WebSocket connections and coordinating communication between the application (frontend and reasoning), STT, and TTS. ","version":"Next","tagName":"h2"},{"title":"Introduction","type":0,"sectionRef":"#","url":"/xrx-core/docs/introduction","content":"","keywords":"","version":"Next"},{"title":"What is xRx?​","type":1,"pageTitle":"Introduction","url":"/xrx-core/docs/introduction#what-is-xrx","content":" Any modality input (x), reasoning (R), any modality output (x).  xRx is a framework for building AI-powered applications that interact with users across multiple modalities, where &quot;x&quot; represents the flexible integration of text, voice, and other interaction forms.  ","version":"Next","tagName":"h2"},{"title":"Project Motivation​","type":1,"pageTitle":"Introduction","url":"/xrx-core/docs/introduction#project-motivation","content":" We believe the way we interact with software is changing and we're excited to be on the forefront of this movement. The xRx repository accelerates the development of multimodal AI applications, going beyond the typical focus on audio experiences. It serves as a foundation for AI-powered reasoning systems capable of engaging users through various modalities, including audio, text, app widgets, and more. This versatility sets xRx apart from most current multimodal AI systems, positioning it as a comprehensive solution for the evolving landscape of software interaction.  ","version":"Next","tagName":"h2"},{"title":"Navigating the Documentation​","type":1,"pageTitle":"Introduction","url":"/xrx-core/docs/introduction#navigating-the-documentation","content":" Our documentation is structured to provide you with a comprehensive understanding of xRx and guide you through its implementation. Here's an overview of what you'll find:  Introduction: You're here now! This section provides an overview of xRx and its core concepts. Quick Start: For those eager to dive in, this section will help you set up and run xRx quickly. Tutorials: Step-by-step guides for various use cases and configurations to help you get the most out of xRx. How It Works: Detailed explanations of xRx's system architecture, components, and underlying principles. References: In-depth documentation on key components like the Agent, Guardrails, TTS, and STT. Contributing: Guidelines for those interested in contributing to the xRx project.  Use the sidebar navigation to move between these sections. If you're looking for something specific, try using the search functionality at the top of the sidebar.  ","version":"Next","tagName":"h2"},{"title":"Contributing​","type":1,"pageTitle":"Introduction","url":"/xrx-core/docs/introduction#contributing","content":" We're an open source project and are extremely open to contributors adding new features, integrations, and documentation! Please don't hesitate to reach out and get started building with us.  For more information on contributing, see our Contribution Guide. ","version":"Next","tagName":"h2"},{"title":"API References","type":0,"sectionRef":"#","url":"/xrx-core/docs/references/","content":"","keywords":"","version":"Next"},{"title":"Components​","type":1,"pageTitle":"API References","url":"/xrx-core/docs/references/#components","content":" OrchestratorSpeech-To-Text (STT)Text-To-Speech (TTS)AgentGuardrails ","version":"Next","tagName":"h2"},{"title":"Guardrails","type":0,"sectionRef":"#","url":"/xrx-core/docs/how-it-works/guardrails","content":"","keywords":"","version":"Next"},{"title":"Key Features​","type":1,"pageTitle":"Guardrails","url":"/xrx-core/docs/how-it-works/guardrails#key-features","content":" Configurable API Proxy: Can be set up to monitor any API endpoint.Guardrails AI Hub Integration: Utilizes a wide range of pre-built and custom guardrails.Portable Design: Can be easily integrated with any existing API.  ","version":"Next","tagName":"h2"},{"title":"How It Works​","type":1,"pageTitle":"Guardrails","url":"/xrx-core/docs/how-it-works/guardrails#how-it-works","content":" ","version":"Next","tagName":"h2"},{"title":"1. API Proxy Configuration​","type":1,"pageTitle":"Guardrails","url":"/xrx-core/docs/how-it-works/guardrails#1-api-proxy-configuration","content":" The Guardrails proxy can be configured to intercept requests for any API endpoint. This is done through a simple configuration object:  MONITOR_CONFIG = { &quot;endpoint-name&quot;: { &quot;input_keys&quot;: { &quot;field_name&quot;: { &quot;type&quot;: &quot;data_type&quot;, // Additional configuration... } } } }   This configuration tells the proxy how to extract and process data from incoming requests, allowing for flexible adaptation to different API structures.  ","version":"Next","tagName":"h3"},{"title":"2. Guardrails AI Hub Integration​","type":1,"pageTitle":"Guardrails","url":"/xrx-core/docs/how-it-works/guardrails#2-guardrails-ai-hub-integration","content":" The proxy leverages the Guardrails AI Hub, which provides a variety of pre-built guardrails such as:  Toxic language detectionPII (Personally Identifiable Information) detectionSentiment analysisLanguage detectionContent classification  Custom guardrails can also be easily implemented and integrated into the system.  ","version":"Next","tagName":"h3"},{"title":"3. Portability​","type":1,"pageTitle":"Guardrails","url":"/xrx-core/docs/how-it-works/guardrails#3-portability","content":" The Guardrails proxy is designed to be easily integrated with any existing API. It can be deployed as a separate service that sits between your clients and your main API, requiring minimal changes to your existing infrastructure.  ","version":"Next","tagName":"h3"},{"title":"Implementation​","type":1,"pageTitle":"Guardrails","url":"/xrx-core/docs/how-it-works/guardrails#implementation","content":" Request Interception: The proxy intercepts incoming API requests.Data Extraction: It extracts relevant data based on the configuration.Guardrails Application: The extracted data is passed through the configured guardrails.Request Forwarding: If all checks pass, the request is forwarded to the main API.Response Handling: The proxy can optionally apply guardrails to the API's response as well.  ","version":"Next","tagName":"h2"},{"title":"Benefits​","type":1,"pageTitle":"Guardrails","url":"/xrx-core/docs/how-it-works/guardrails#benefits","content":" Enhanced Security: Protect your API from malicious or inappropriate content.Flexibility: Easy to configure for different endpoints and data structures.Scalability: Leverage powerful AI models without burdening your main API.Customization: Implement custom guardrails for your specific needs.  By utilizing the Guardrails proxy, you can add a robust layer of protection and moderation to any API, enhancing the security and reliability of your applications. ","version":"Next","tagName":"h2"},{"title":"Graph Agents: Shopify app","type":0,"sectionRef":"#","url":"/xrx-core/docs/how-it-works/graph_agents_shopify_app","content":"","keywords":"","version":"Next"},{"title":"Designing Human Interactions​","type":1,"pageTitle":"Graph Agents: Shopify app","url":"/xrx-core/docs/how-it-works/graph_agents_shopify_app#designing-human-interactions","content":" When designing and developing this agent, other than table stakes software principles (modularity, cost, etc.) the core design principle we strive for with xRx is providing &quot;human-like interaction&quot;.  The Shopify application is designed to be highly interactive to the end user in the xRx system. Chatting with LLMs text does not cut it for passing the bar of &quot;human-like&quot; outside of trivial interfaces. By including voice in the experience, an AI system can sound like a human, but the interactions tend to miss common nuances about how people like us have a normal conversation in a customer support setting.  Here are a few ways we addressed this design principle...  Groq for Inference  A common complaint with agent systems in applications is that they are &quot;too slow&quot; for real time user interaction. This tends to happen for a few reasons. First is inference speed for LLMs capable of reasoning through problems. We try and reduce this bottleneck as much as possible by using Groq for extremely fast inference.  Acknowledgement of Actions  Let's take an example to explain this: Ordering food at a restaurant. When you are ordering, there are many queues that humans pick up on that the waiter is performing actions to help you. For instance, you might see the waiter write down your order; the waiter might nod when you ask for an alteration; the waiter might run to the back to ask the kitchen a question. You innately understand these actions without the need for the waiter to explain them to you each time.  The core problem boils down to a simple question: How do you embed these human queues into a software powered by an LLM?  Our Shopify agent tackles this challenge through visual and audio queues. First, when requests are going to be using tools, the agent has a node in the graph which runs in parallel to the tool calling and analysis which is only responsible for telling the user what is going on through an audio modality. The need for this node arises from the need for immediate responses to the end user when multiple turns on an LLM are going to need to be executed in order to respond to what the user said. Even with using Groq for inference, these requests can take many seconds to run.  Second, when specific tools are called by the agent, the xRx system will display visual widgets to the user in the front end which are relevant to whatever the user said. For example, if a customer asks &quot;what kind of wing sauces do you have&quot;, it is a much better experience to simply display a list with prices compared to listing them off via audio.  The overall flow of information might display as follows...  User via Audio: What kind of wing sauces do you have?Assistant via Audio: One sec I'll check our options todayAssistant via App Visual: [widget appears on screen with options]Assistant via Audio: Any of these look good to you?User via Audio: Yeah I'll get 6 lemon garlic and 12 hot buffaloAssistant via Audio: Let me add those to your order quickAssistant via App Visual: [widget appears on screen with current order including items and price]Assistant via Audio: Anything else today?  Multiple User Interaction Paradigms  When thinking about how common ways users interact with applications, the vast majority of interactions are simply &quot;clicks&quot;. Our Shopify agent strives to take advantage of this easy way to interact with a language model by allowing users not only to speak to the reasoning system, but also have the reasoning system interpret the actions which the user takes in the application. A simple way users can interact with the Shopify agent is by clicking on menu items to add them to their order when the agent provides menu items. For example, the flow above might change to...  User via Audio: What kind of wing sauces do you have?Assistant via Audio: One sec I'll check our options todayAssistant via App Visual: [widget appears on screen with options]Assistant via Audio: Any of these look good to you?User via Click: [clicks 6 lemon garlic wings]Assistant via App Visual: [widget appears on screen with current order including items and price]Assistant via Audio: Anything else today?  ","version":"Next","tagName":"h2"},{"title":"Technical Implementation​","type":1,"pageTitle":"Graph Agents: Shopify app","url":"/xrx-core/docs/how-it-works/graph_agents_shopify_app#technical-implementation","content":" Now lets dive into how this all works on the backend. The Shopify reasoning agent is a tool calling agent which has been well documented in the LLM agent space. The execution paradigm is an asynchronous graph where each node determines which subsequent node(s) to traverse to based on its outcome. These nodes might be a single call to an LLM, multiple calls to an LLM, or simple python code which calls no LLMs.    Let's dive into the specifics of how these components are constructed...  ","version":"Next","tagName":"h2"},{"title":"Graph Execution​","type":1,"pageTitle":"Graph Agents: Shopify app","url":"/xrx-core/docs/how-it-works/graph_agents_shopify_app#graph-execution","content":" The graph structure follows the flow which is shown in the diagram below. Each node is a different process. The red notes like &quot;Routing&quot; call an LLM. The grey nodes like &quot;Widget&quot; are simple python functions which do not call an LLM.    Graph definition​  We have used the networkx Python library for our graph implementation. It provides a low level graph abstraction where we do not have to rewrite how to perform graph operations in Python. However, it provides enough flexibility to pass information between nodes without any inhibitions.  The base implementation of the graph classes can be viewed here.  Nodes​  Each node in the agent graph is defined with two required functions.  The process function is the function which executes when the node is triggers. This is where any calls to an LLM are located. The output of the process function will then yield an output dictionary with will contain an &quot;output&quot; which will determine the next node to traverse to.  After the process function is called a get_successors node is called in order to determine what the next actions are. The get_successors function will return a list of node names which should be triggered and any inputs which should be sent to those nodes.  The nodes which are used in the reasoning system are available here.  Here is the list of nodes which are available in the graph today.  Routing: Determines the next action based on the current state and conversation context.TaskDescriptionResponse: Generates a brief, personalized waiting message for the customer.ChooseTool: Selects the appropriate tool to use based on the current task.IdentifyToolParams: Identifies and prepares the parameters needed for the selected tool.ExecuteTool: Executes the selected tool with the prepared parameters.Widget: Displays relevant visual widgets to the user in the front end.ConvertNaturalLanguage: Converts tool outputs into natural language descriptions.  ","version":"Next","tagName":"h3"},{"title":"Tools​","type":1,"pageTitle":"Graph Agents: Shopify app","url":"/xrx-core/docs/how-it-works/graph_agents_shopify_app#tools","content":" All the tools available to the agent are defined as simple python functions which reach out to Shopify via the Python SDK. These tools can be seen here. They follow the general code format shown below.  @observability_decorator(name='my_tool') def my_tool(arg1: str): &quot;&quot;&quot; Doc string with description, when to use, arguments, and outputs &quot;&quot;&quot; your_code = '' return your_code   It is very important to note that these doc strings are used in the prompt for the agent. This means that any change to the doc string will impact the performance of the reasoning agent.  Please read the observability tutorial for more information regarding the observability decorator above.  ","version":"Next","tagName":"h3"},{"title":"Passing Messages to the Frontend​","type":1,"pageTitle":"Graph Agents: Shopify app","url":"/xrx-core/docs/how-it-works/graph_agents_shopify_app#passing-messages-to-the-frontend","content":" The three nodes which are used to pass messages to the frontend are Widget, CustomerResponse, and TaskDescriptionResponse. These node names recognized by the orchestrator to determine which information the agent creates which needs to be passed to the frontend. As the graph is executed, the FastAPI streaming response will yield a dictionary which contains two specific fields.  node: The name of the node which generated the output below.output: The output of the node which was executed. In the case of Widget, this is a JSON object which is then mapped to a rendering in the front end. In the case of the other nodes, this is a simple string which is passed to the frontend to be played as audio.  ","version":"Next","tagName":"h3"},{"title":"Conversation Context​","type":1,"pageTitle":"Graph Agents: Shopify app","url":"/xrx-core/docs/how-it-works/graph_agents_shopify_app#conversation-context","content":" For all of the nodes, when they are executed, their output is streamed to the orchestrator container. The orchestrator container is in charge of maintaining conversational context as it happens in real time. Any information which should be persisted in the conversation history will be passed out in the messages field which is streamed out of the reasoning container.  There is some heavy string manipulation which happens in the reasoning system to maintain a record of this conversation history because of the various modalities which are used to interact with the user. Specifically, all the tool calls which need to be kept track of are stored in a list which is then joined into a single string and passed into the system prompt.  ","version":"Next","tagName":"h3"},{"title":"Context Handling​","type":1,"pageTitle":"Graph Agents: Shopify app","url":"/xrx-core/docs/how-it-works/graph_agents_shopify_app#context-handling","content":" Because the Shopify system has a session associated with the application and the user, it is important to maintain a record of the session ID and the cart ID (if there is one) in order to ensure that the user's order is being tracked correctly. This is done by the orchestrator passing the session ID and cart ID into the FastAPI endpoint which serves the reasoning agent. All inputs to the reasoning agent will have the following json structure...  { &quot;session&quot;: { &quot;session_id&quot;: &quot;session_id&quot;, &quot;cart_id&quot;: &quot;cart_id&quot; }, &quot;messages&quot;: [ { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;...&quot; }, { &quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;...&quot; }, { &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;...&quot; } ] }   Once the orchestrator passes in the session and messages, the reasoning agent will use the messages field to determine the conversation history and the session field to determine the session ID and cart ID. The session variable is then stored per execution here via the context_manager module.  It is important to note that we could pass the session into each tool call. However, because each tool definition is passed into the prompt for some of the nodes in the graph, it is better to handle the session in the context manager as to not introduce more inputs to the tool call function which need to be understood by the LLM.  Here is an example of how the session data is set and retrieved within a given context. Notice how my_function can be called and retrieve the session_data without needing an explicit input argument.  from context_manager import set_session, session_var def my_session_function(): session_data = session_var.get() print(session_data['session_id']) session_data[&quot;cart_id&quot;] = 5678 session_var.set(session_data) return None session_data = { &quot;session_id&quot;: 1234 } with set_session(session_data): _ = my_function() print(session_var.get())   1234 {'session_id': 1234, 'cart_id': 5678}   For a more detailed look at this being used in action, check out the tools which are available here.  ","version":"Next","tagName":"h3"},{"title":"Canceling an Ongoing Task​","type":1,"pageTitle":"Graph Agents: Shopify app","url":"/xrx-core/docs/how-it-works/graph_agents_shopify_app#canceling-an-ongoing-task","content":" The Shopify reasoning agent supports canceling an ongoing task, which is crucial for providing a responsive user experience and managing system resources efficiently. This functionality is implemented using Redis to track the status of each task.  Task Execution and Cancellation Flow​  When a new task is initiated: A unique task_id is generated using UUID.The task status is set to 'running' in Redis.The task_id is included in the response headers as 'X-Task-ID'. To cancel a task: A POST request is sent to the /cancel-reasoning-agent/{task_id} endpoint.The task status in Redis is updated to 'cancelled'. During execution: The agent periodically checks the task status in Redis.If the status is 'cancelled', the agent will stop processing and gracefully terminate the task.  Redis Integration​  The system uses a Redis container (xrx-redis) to store and manage task statuses. This allows for efficient, real-time status updates and checks across the distributed system.  If you are using the docker-compose setup, the Redis container will be automatically started and the reasoning agent will be able to use it as long as the environment variable is correctly set as shown below.  REDIS_HOST=&quot;xrx-redis&quot;   If you are running the agent locally outside of docker compose, the reasoning agent will look for a Redis container on the default host (localhost) and port (6379). In order to start that server, you can use the following command:  docker run -d --name redis-server -p 6379:6379 redis   API Usage Example​  To cancel a task, send a POST request to the cancellation endpoint:  POST /cancel-reasoning-agent/{task_id}   Example response:  { &quot;detail&quot;: &quot;Task {task_id} cancelled&quot; }   If successful, this will return a 200 status code. In case of an error, it will return a 500 status code with an error message.  Implementation Details​  The cancellation mechanism is implemented in the FastAPI application:  @app.post(&quot;/cancel-reasoning-agent/{task_id}&quot;) async def cancel_agent(task_id: str): try: await redis_client.set(task_id, 'cancelled') logger.info(f&quot;Task {task_id} set to cancelled&quot;) return JSONResponse(content={&quot;detail&quot;: f&quot;Task {task_id} cancelled&quot;}, status_code=200) except Exception as e: logger.error(f&quot;An error occurred while cancelling the task: {str(e)}&quot;) return JSONResponse(content={&quot;detail&quot;: f&quot;An error occurred: {str(e)}&quot;}, status_code=500)   This cancellation feature enhances the robustness of the Shopify reasoning agent, allowing for better control over task execution and improved user experience. It's particularly useful for long-running tasks or when the user decides to change their request mid-process. ","version":"Next","tagName":"h3"},{"title":"Quick Start","type":0,"sectionRef":"#","url":"/xrx-core/docs/quickstart","content":"","keywords":"","version":"Next"},{"title":"Introduction​","type":1,"pageTitle":"Quick Start","url":"/xrx-core/docs/quickstart#introduction","content":" In this quick start, we will walk through the steps to get the xRx system up and running. xRx is designed as a modular system with pluggable components, with the core consisting of the reasoning agent. You only need to code a reasoning flow, and xRx will automatically create an application that can listen, speak, and display custom UI components.  For this guide, we will be using the simple agent as our example, but the setup process remains consistent for any reasoning agent you choose to implement.  ","version":"Next","tagName":"h2"},{"title":"Components​","type":1,"pageTitle":"Quick Start","url":"/xrx-core/docs/quickstart#components","content":" The xRx system is designed with a modular architecture, separating the core framework from specific applications. This design allows for greater flexibility and customization in application development.  Application​  The application layer consists of a Next.js front-end client and a reasoning agent, working together to process user inputs and generate responses through interaction with the xRx Core components.  xRx Core​  The xRx Core provides the foundation of the system, including an orchestrator for managing communications, TTS and STT services for audio processing, a React client library for UI development, and an agent framework for building reasoning agents. These components are designed to be reusable across different applications.  This modular structure allows developers to easily customize and extend xRx for their specific needs while benefiting from a solid, tested foundation.  ","version":"Next","tagName":"h2"},{"title":"Getting Started​","type":1,"pageTitle":"Quick Start","url":"/xrx-core/docs/quickstart#getting-started","content":" ","version":"Next","tagName":"h2"},{"title":"Prerequisites​","type":1,"pageTitle":"Quick Start","url":"/xrx-core/docs/quickstart#prerequisites","content":" To deploy the xRx system locally, you'll need to install a few dependencies. If you're using macOS, you can install them with the following:  If you don't have Homebrew installed, you'll need to install it:  /bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot;   Add Homebrew to your PATH. For bash:  echo &quot;export PATH=/usr/local/bin:$PATH&quot; &gt;&gt; ~/.bash_profile &amp;&amp; source ~/.bash_profile   For zsh:  echo &quot;export PATH=/usr/local/bin:$PATH&quot; &gt;&gt; ~/.zshrc &amp;&amp; source ~/.zshrc   Verify the installation:  brew --version   Install Docker using Homebrew:  brew install --cask docker   Verify the Docker installation:  docker --version   Start Docker:  open -a Docker   ","version":"Next","tagName":"h3"},{"title":"Clone the xRx Repository​","type":1,"pageTitle":"Quick Start","url":"/xrx-core/docs/quickstart#clone-the-xrx-repository","content":" Clone the repository with its submodules using the following command:  git clone --recursive https://github.com/8090-inc/xrx-sample-apps.git   Navigate to the cloned repository:  cd xrx-sample-apps   Note: The --recursive flag is crucial here. It ensures that you also clone the xrx-core submodule, which contains the fundamental building blocks of the xRx framework. Without this, your project won't have access to the core functionalities it needs.  If you've already cloned the repository without the --recursive flag, or if you need to update the submodule later, you can use:  git submodule update --init --recursive   ","version":"Next","tagName":"h2"},{"title":"External Services Configuration​","type":1,"pageTitle":"Quick Start","url":"/xrx-core/docs/quickstart#external-services-configuration","content":" xRx requires three external services: LLM, Text-to-Speech, and Speech-to-Text. Configure these services by setting the environment variables in the .env file inside the simple-agent application.  ","version":"Next","tagName":"h2"},{"title":"LLMs​","type":1,"pageTitle":"Quick Start","url":"/xrx-core/docs/quickstart#llms","content":" We recommend Groq for high token throughput. Sign up at Groq and obtain an API key.  LLM_API_KEY=&lt;your groq api key&gt; LLM_BASE_URL=&quot;https://api.groq.com/openai/v1&quot; LLM_MODEL_ID=&quot;llama3-70b-8192&quot;   We recommend the models in the variables above for our repository, but they can be changed to any model that is supported by the LLM provider.  ","version":"Next","tagName":"h3"},{"title":"Speech to Text​","type":1,"pageTitle":"Quick Start","url":"/xrx-core/docs/quickstart#speech-to-text","content":" Multiple transcription options are supported in xRx. Currently, you can use OpenAI's Whisper model running on Groq Cloud, Deepgram, or a local whisper model. For the easiest setup, we recommend using Groq's Whisper because you will already have an API key from the previous step. If you want to use two different Groq API keys, that is supported as well.  Set the following environment variables for the STT service in your .env:  STT_PROVIDER=&quot;groq&quot; GROQ_STT_API_KEY=&quot;&lt;your groq api key&gt;&quot;   ","version":"Next","tagName":"h3"},{"title":"Text to Speech​","type":1,"pageTitle":"Quick Start","url":"/xrx-core/docs/quickstart#text-to-speech","content":" The xRx TTS module supports multiple models for text-to-speech conversion. For this demonstration, we will utilize Elevenlabs, known for its high-quality voice synthesis. Register at Elevenlabs and obtain an API key. Update the .env file as seen below:  TTS_PROVIDER=&quot;elevenlabs&quot; ELEVENLABS_API_KEY=&lt;your elevenlabs api key&gt; ELEVENLABS_VOICE_ID=&lt;your elevenlabs voice id&gt;   ","version":"Next","tagName":"h3"},{"title":"How To Run the Simple Agent application​","type":1,"pageTitle":"Quick Start","url":"/xrx-core/docs/quickstart#how-to-run-the-simple-agent-application","content":" Navigate to the Simple Agent directory within the applications folder. Inside this directory, locate the example .env file and input your API keys. Follow the instructions in the Readme.md file to install the necessary requirements. Build and run the system using Docker:  docker-compose up --build   Access the xRx Demo client at http://localhost:3000  Enjoy exploring and interacting with the xRx system!  ","version":"Next","tagName":"h2"},{"title":"Contributing​","type":1,"pageTitle":"Quick Start","url":"/xrx-core/docs/quickstart#contributing","content":" We welcome contributions to the xRx framework and its sample applications. If you have any suggestions or improvements, please follow these steps:  Open a new issue on GitHub describing the proposed change or improvementFork the repositoryCreate a new branch for your featureCommit your changesPush to your branchCreate a pull request, referencing the issue you created  Note: Pull requests not backed by published issues will not be considered. This process ensures that all contributions are discussed and aligned with the project's goals before implementation. ","version":"Next","tagName":"h2"},{"title":"Reasoning Agent","type":0,"sectionRef":"#","url":"/xrx-core/docs/references/agent","content":"","keywords":"","version":"Next"},{"title":"Endpoints​","type":1,"pageTitle":"Reasoning Agent","url":"/xrx-core/docs/references/agent#endpoints","content":" ","version":"Next","tagName":"h2"},{"title":"Execute Agent​","type":1,"pageTitle":"Reasoning Agent","url":"/xrx-core/docs/references/agent#execute-agent","content":" POST /run-reasoning-agent  Execute the reasoning agent and stream the results.  This endpoint starts a new task for the reasoning agent and returns a streaming response with the agent's output. The task ID is returned in the 'X-Task-ID' header of the response.  Request Body​  Field\tType\tDescriptionmessages\tarray of Message\tList of messages in the conversation session\tobject\tSession information for the conversation action\tobject or null\tOptional action to be performed by the agent (default: )  Responses​  200: Successful Response422: Validation Error  ","version":"Next","tagName":"h3"},{"title":"Cancel Agent​","type":1,"pageTitle":"Reasoning Agent","url":"/xrx-core/docs/references/agent#cancel-agent","content":" POST /cancel-reasoning-agent/{task_id}  Cancel a running reasoning agent task.  This endpoint attempts to cancel a task identified by the given task_id. It sets the task status to 'cancelled' in Redis.  Parameters​  Name\tIn\tType\tRequired\tDescriptiontask_id\tpath\tstring\tYes\tThe ID of the task to cancel  Responses​  200: Successful Response Content: CancelResponse 422: Validation Error  ","version":"Next","tagName":"h3"},{"title":"Schemas​","type":1,"pageTitle":"Reasoning Agent","url":"/xrx-core/docs/references/agent#schemas","content":" ","version":"Next","tagName":"h2"},{"title":"AgentRequest​","type":1,"pageTitle":"Reasoning Agent","url":"/xrx-core/docs/references/agent#agentrequest","content":" Field\tType\tRequired\tDescriptionmessages\tarray of Message\tYes\tList of messages in the conversation session\tobject\tNo\tSession information for the conversation action\tobject or null\tNo\tOptional action to be performed by the agent (default: )  ","version":"Next","tagName":"h3"},{"title":"CancelResponse​","type":1,"pageTitle":"Reasoning Agent","url":"/xrx-core/docs/references/agent#cancelresponse","content":" Field\tType\tRequireddetail\tstring\tYes  ","version":"Next","tagName":"h3"},{"title":"Message​","type":1,"pageTitle":"Reasoning Agent","url":"/xrx-core/docs/references/agent#message","content":" Field\tType\tRequiredrole\tstring\tYes content\tstring\tYes  ","version":"Next","tagName":"h3"},{"title":"HTTPValidationError​","type":1,"pageTitle":"Reasoning Agent","url":"/xrx-core/docs/references/agent#httpvalidationerror","content":" Field\tType\tDescriptiondetail\tarray of ValidationError\t  ","version":"Next","tagName":"h3"},{"title":"ValidationError​","type":1,"pageTitle":"Reasoning Agent","url":"/xrx-core/docs/references/agent#validationerror","content":" Field\tType\tDescriptionloc\tarray of (string or integer)\tLocation msg\tstring\tMessage type\tstring\tError Type ","version":"Next","tagName":"h3"},{"title":"Guardrails","type":0,"sectionRef":"#","url":"/xrx-core/docs/references/guardrails","content":"","keywords":"","version":"Next"},{"title":"Info​","type":1,"pageTitle":"Guardrails","url":"/xrx-core/docs/references/guardrails#info","content":" Title: Guardrails ProxyDescription: A proxy server that applies guardrails to requests and responses.Version: 1.0.0  ","version":"Next","tagName":"h2"},{"title":"Paths​","type":1,"pageTitle":"Guardrails","url":"/xrx-core/docs/references/guardrails#paths","content":" ","version":"Next","tagName":"h2"},{"title":"/ {path}​","type":1,"pageTitle":"Guardrails","url":"/xrx-core/docs/references/guardrails#-path","content":" PUT​  Summary: Proxy Request  Description: Proxies incoming requests to the target server, applying guardrails if configured.  Args:  path (str): The path of the incoming request.request (Request): The incoming FastAPI request object.  Returns: Response: The proxied response, potentially modified by guardrails.  Parameters:  path (string, required): Path  Responses:  200: Successful Response422: Validation Error  GET​  Summary: Proxy Request  Description: Proxies incoming requests to the target server, applying guardrails if configured.  Args:  path (str): The path of the incoming request.request (Request): The incoming FastAPI request object.  Returns: Response: The proxied response, potentially modified by guardrails.  Parameters:  path (string, required): Path  Responses:  200: Successful Response422: Validation Error  PATCH​  Summary: Proxy Request  Description: Proxies incoming requests to the target server, applying guardrails if configured.  Args:  path (str): The path of the incoming request.request (Request): The incoming FastAPI request object.  Returns: Response: The proxied response, potentially modified by guardrails.  Parameters:  path (string, required): Path  Responses:  200: Successful Response422: Validation Error  OPTIONS​  Summary: Proxy Request  Description: Proxies incoming requests to the target server, applying guardrails if configured.  Args:  path (str): The path of the incoming request.request (Request): The incoming FastAPI request object.  Returns: Response: The proxied response, potentially modified by guardrails.  Parameters:  path (string, required): Path  Responses:  200: Successful Response422: Validation Error  POST​  Summary: Proxy Request  Description: Proxies incoming requests to the target server, applying guardrails if configured.  Args:  path (str): The path of the incoming request.request (Request): The incoming FastAPI request object.  Returns: Response: The proxied response, potentially modified by guardrails.  Parameters:  path (string, required): Path  Responses:  200: Successful Response422: Validation Error  DELETE​  Summary: Proxy Request  Description: Proxies incoming requests to the target server, applying guardrails if configured.  Args:  path (str): The path of the incoming request.request (Request): The incoming FastAPI request object.  Returns: Response: The proxied response, potentially modified by guardrails.  Parameters:  path (string, required): Path  Responses:  200: Successful Response422: Validation Error  ","version":"Next","tagName":"h3"},{"title":"Components​","type":1,"pageTitle":"Guardrails","url":"/xrx-core/docs/references/guardrails#components","content":" ","version":"Next","tagName":"h2"},{"title":"Schemas​","type":1,"pageTitle":"Guardrails","url":"/xrx-core/docs/references/guardrails#schemas","content":" HTTPValidationError​  Type: objectTitle: HTTPValidationError  Properties:  detail (array of ValidationError): Detail  ValidationError​  Type: objectRequired: [&quot;loc&quot;, &quot;msg&quot;, &quot;type&quot;]Title: ValidationError  Properties:  loc (array of string or integer): Locationmsg (string): Messagetype (string): Error Type ","version":"Next","tagName":"h3"},{"title":"Tutorials","type":0,"sectionRef":"#","url":"/xrx-core/docs/tutorials/","content":"","keywords":"","version":"Next"},{"title":"Available Tutorials​","type":1,"pageTitle":"Tutorials","url":"/xrx-core/docs/tutorials/#available-tutorials","content":" ","version":"Next","tagName":"h2"},{"title":"Reasoning​","type":1,"pageTitle":"Tutorials","url":"/xrx-core/docs/tutorials/#reasoning","content":" Quick StartRun the Shopify ApplicationRun the Patient Intake ApplicationBuild Your Own Reasoning Application  ","version":"Next","tagName":"h3"},{"title":"Guardrails and Observabililty​","type":1,"pageTitle":"Tutorials","url":"/xrx-core/docs/tutorials/#guardrails-and-observabililty","content":" Enabling LLM GuardrailsSetting up LLM Observability  ","version":"Next","tagName":"h3"},{"title":"Integrations​","type":1,"pageTitle":"Tutorials","url":"/xrx-core/docs/tutorials/#integrations","content":" Changing STT ProvidersChanging TTS Providers  ","version":"Next","tagName":"h3"},{"title":"Front End​","type":1,"pageTitle":"Tutorials","url":"/xrx-core/docs/tutorials/#front-end","content":" Creating your own WidgetsUsing the UI Debug Mode ","version":"Next","tagName":"h3"},{"title":"Orchestrator","type":0,"sectionRef":"#","url":"/xrx-core/docs/references/orchestrator","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Orchestrator","url":"/xrx-core/docs/references/orchestrator#overview","content":" The xRx WebSocket API allows clients to connect to a server for real-time communication. The server handles both text and binary messages, providing functionalities such as speech-to-text (STT), text-to-speech (TTS), and interaction with an agent.  ","version":"Next","tagName":"h3"},{"title":"Endpoint​","type":1,"pageTitle":"Orchestrator","url":"/xrx-core/docs/references/orchestrator#endpoint","content":" URL: /api/v1/wsMethod: GETProtocol: WebSocket  ","version":"Next","tagName":"h3"},{"title":"Authentication​","type":1,"pageTitle":"Orchestrator","url":"/xrx-core/docs/references/orchestrator#authentication","content":" Header: x-api-keyValue: API_KEY from environment variables (default: 123456)  ","version":"Next","tagName":"h3"},{"title":"Connection​","type":1,"pageTitle":"Orchestrator","url":"/xrx-core/docs/references/orchestrator#connection","content":" Upon connecting to the WebSocket endpoint, the server will log the connection and store the session.  ","version":"Next","tagName":"h3"},{"title":"Messages​","type":1,"pageTitle":"Orchestrator","url":"/xrx-core/docs/references/orchestrator#messages","content":" The server handles two types of messages: text and binary.  Text Messages​  Format: JSONFields: type: &quot;text&quot; or &quot;action&quot;content: The text content or action details  Example:  { &quot;type&quot;: &quot;text&quot;, &quot;content&quot;: &quot;Hello, how can I help you?&quot; }   Binary Messages​  Format: Binary data (e.g., audio data)  ","version":"Next","tagName":"h3"},{"title":"Events​","type":1,"pageTitle":"Orchestrator","url":"/xrx-core/docs/references/orchestrator#events","content":" message​  Description: Triggered when a message is received from the client.Parameters: message: The message data (text or binary)isBinary: Boolean indicating if the message is binary  Handling Text Messages:  If type is &quot;text&quot;, the server will append the text to the session's chat history and send it to the agent.If type is &quot;action&quot;, the server will send the action to the agent.  Handling Binary Messages:  The server will append the binary data (audio) to the session's speech buffer and process it using VAD (Voice Activity Detection).  close​  Description: Triggered when the client closes the connection.Parameters: code: The close codereason: The reason for closing  ","version":"Next","tagName":"h3"},{"title":"Server Responses​","type":1,"pageTitle":"Orchestrator","url":"/xrx-core/docs/references/orchestrator#server-responses","content":" Text Responses​  Format: JSONFields: user: The user who sent the messagecontent: The message contenttype: The type of message (&quot;text&quot;, &quot;audio&quot;, &quot;widget&quot;)  Example:  { &quot;user&quot;: &quot;agent&quot;, &quot;content&quot;: &quot;One moment please...&quot;, &quot;type&quot;: &quot;text&quot; }   Binary Responses​  Format: Binary data (e.g., audio data)  ","version":"Next","tagName":"h3"},{"title":"Error Handling​","type":1,"pageTitle":"Orchestrator","url":"/xrx-core/docs/references/orchestrator#error-handling","content":" If the session is not found, the server logs an error and does not process the message.If the STT or TTS WebSocket is not open, the server attempts to reopen the connection and logs any errors.  ","version":"Next","tagName":"h3"},{"title":"Environment Variables​","type":1,"pageTitle":"Orchestrator","url":"/xrx-core/docs/references/orchestrator#environment-variables","content":" API_KEY: API key for authenticationAGENT_HOST, AGENT_PORT: Configuration for the agentGUARDRAILS_AGENT_HOST, GUARDRAILS_AGENT_PORT: Configuration for the guardrails agentAGENT_WAIT_MS, SAMPLE_RATE, STT_WAIT_MS: Timing and sample rate configurations for the STTSTT_HOST, STT_PORT, STT_PATH: Configuration for the STT WebSocketTTS_HOST, TTS_PORT, TTS_PATH: Configuration for the TTS WebSocketINITIAL_RESPONSE: Initial response messageSTT_PROVIDER: STT provider (e.g., deepgram)REDIS_HOST: Configuration for Redis  ","version":"Next","tagName":"h3"},{"title":"Example Usage​","type":1,"pageTitle":"Orchestrator","url":"/xrx-core/docs/references/orchestrator#example-usage","content":" Connecting to the WebSocket:  const socket = new WebSocket('ws://localhost:8000/api/v1/ws'); socket.onopen = () =&gt; { console.log('Connection opened'); }; socket.onmessage = (event) =&gt; { if (typeof event.data === 'string') { const message = JSON.parse(event.data); console.log('Received message:', message); } else { console.log('Received binary data'); } }; socket.onclose = (event) =&gt; { console.log(`Connection closed: ${event.code} ${event.reason}`); }; socket.onerror = (error) =&gt; { console.error('WebSocket error:', error); };   Sending a Text Message:  const message = { type: 'text', content: 'Hello, how can I help you?' }; socket.send(JSON.stringify(message));   Sending a Binary Message:  const audioData = new Uint8Array([/* audio data */]); socket.send(audioData);   This documentation provides an overview of the WebSocket API, including connection details, message formats, events, and example usage. ","version":"Next","tagName":"h3"},{"title":"Changing STT providers","type":0,"sectionRef":"#","url":"/xrx-core/docs/tutorials/changing_stt_providers","content":"","keywords":"","version":"Next"},{"title":"Supported STT Providers​","type":1,"pageTitle":"Changing STT providers","url":"/xrx-core/docs/tutorials/changing_stt_providers#supported-stt-providers","content":" Currently, xRx supports the following STT providers:  DeepgramGroq's WhisperLocal Whisper model  ","version":"Next","tagName":"h2"},{"title":"Configuring the STT Provider​","type":1,"pageTitle":"Changing STT providers","url":"/xrx-core/docs/tutorials/changing_stt_providers#configuring-the-stt-provider","content":" To change the STT provider, you need to modify the environment variables in your .env file. Here's how to do it:  Open your .env file located at the root of the xRx repository. Locate the STT-related environment variables:  # === Speech-to-text options === # STT provider. Choices are &quot;groq&quot;, &quot;deepgram&quot;, or &quot;faster_whisper&quot; STT_PROVIDER=&quot;deepgram&quot; STT_SAMPLE_RATE=&quot;16000&quot; # Deepgram DG_API_KEY=&quot;&lt;Deepgram API key&gt;&quot;   Change the STT_PROVIDER value to your desired provider: For Deepgram: STT_PROVIDER=&quot;deepgram&quot;For Groq's Whisper: STT_PROVIDER=&quot;groq&quot;For local Whisper model: STT_PROVIDER=&quot;faster_whisper&quot; Provide the necessary API keys or configuration for your chosen provider.  ","version":"Next","tagName":"h2"},{"title":"Provider-Specific Configuration​","type":1,"pageTitle":"Changing STT providers","url":"/xrx-core/docs/tutorials/changing_stt_providers#provider-specific-configuration","content":" ","version":"Next","tagName":"h2"},{"title":"Deepgram​","type":1,"pageTitle":"Changing STT providers","url":"/xrx-core/docs/tutorials/changing_stt_providers#deepgram","content":" For Deepgram, you need to set the DG_API_KEY environment variable:  DG_API_KEY=&quot;&lt;your Deepgram API key&gt;&quot;   You can sign up for a Deepgram account and obtain an API key at https://console.deepgram.com/signup.  ","version":"Next","tagName":"h3"},{"title":"Groq's Whisper​","type":1,"pageTitle":"Changing STT providers","url":"/xrx-core/docs/tutorials/changing_stt_providers#groqs-whisper","content":" For Groq's Whisper, you need to set the Groq API key:  GROQ_STT_API_KEY=&quot;&lt;your Groq API key&gt;&quot;   Please note, this can be the same or different API key you use for the LLM API.  ","version":"Next","tagName":"h3"},{"title":"Local Whisper Model​","type":1,"pageTitle":"Changing STT providers","url":"/xrx-core/docs/tutorials/changing_stt_providers#local-whisper-model","content":" For the local Whisper model (faster_whisper), no additional API keys are required. However, ensure that you have the necessary dependencies installed in your environment.  ","version":"Next","tagName":"h3"},{"title":"Applying Changes​","type":1,"pageTitle":"Changing STT providers","url":"/xrx-core/docs/tutorials/changing_stt_providers#applying-changes","content":" After modifying the .env file:  Save the changes to your .env file.Restart your xRx system for the changes to take effect:  docker-compose down docker-compose up --build   ","version":"Next","tagName":"h2"},{"title":"Troubleshooting​","type":1,"pageTitle":"Changing STT providers","url":"/xrx-core/docs/tutorials/changing_stt_providers#troubleshooting","content":" If you encounter issues after changing the STT provider:  Double-check your .env file to ensure all required variables are set correctly.Verify that you have the necessary API keys and they are valid.Check the logs of the STT service for any error messages:  docker-compose logs xrx-stt   Ensure that your chosen STT provider is properly configured and accessible from your deployment environment.  By following these steps, you can easily switch between different STT providers in your xRx system, allowing you to choose the best option for your specific use case and requirements. ","version":"Next","tagName":"h2"},{"title":"Build Your Own Reasoning Application","type":0,"sectionRef":"#","url":"/xrx-core/docs/tutorials/bring_your_own_reasoning_agent","content":"","keywords":"","version":"Next"},{"title":"Clone the repository​","type":1,"pageTitle":"Build Your Own Reasoning Application","url":"/xrx-core/docs/tutorials/bring_your_own_reasoning_agent#clone-the-repository","content":" First, let's get the code onto your local machine:  git clone --recursive https://github.com/8090-inc/xrx-sample-apps.git cd xrx-sample-apps/   Note: The --recursive flag is crucial here. It ensures that you also clone the xrx-core submodule, which contains the fundamental building blocks of the xRx framework. Without this, your project won't have access to the core functionalities it needs.  If you need to update the submodule later (for instance, if there have been updates to the core framework), you can use:  git submodule update --init --recursive   ","version":"Next","tagName":"h2"},{"title":"Rename the simple app directory​","type":1,"pageTitle":"Build Your Own Reasoning Application","url":"/xrx-core/docs/tutorials/bring_your_own_reasoning_agent#rename-the-simple-app-directory","content":" The simple app code resides in the simple-app directory. However, as you're building your own agent, it's a good idea to give it a more descriptive name. This helps in organizing your projects, especially if you plan to create multiple agents.  For example, if you're building a customer support agent:  mv simple-app customer-support-agent   ","version":"Next","tagName":"h2"},{"title":"Implement your reasoning agent​","type":1,"pageTitle":"Build Your Own Reasoning Application","url":"/xrx-core/docs/tutorials/bring_your_own_reasoning_agent#implement-your-reasoning-agent","content":" The heart of your agent lives in the file customer-support-agent/app/agent/executor.py. This is where you'll implement the core logic of your reasoning agent.  The key function here is single_turn_agent. It's called each time a request is sent to your reasoning agent, forming the backbone of your agent's interaction loop.  Here's a simplified version of what this function might look like:  from agent.utils.llm import initialize_llm_client llm_client = initialize_llm_client() def single_turn_agent(messages: List[dict]) -&gt; str: resp = llm_client.chat.completions.create( model=os.environ['LLM_MODEL_ID'], messages=messages, temperature=0.0, ) message = { &quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: resp.choices[0].message.content } out = { &quot;messages&quot;: [message], &quot;node&quot;: &quot;CustomerResponse&quot;, &quot;output&quot;: message['content'], } return out   This function takes in a list of message dictionaries, sends them to a language model, and returns the model's response. The out dictionary is crucial - it contains the response message, specifies the node type (in this case, &quot;CustomerResponse&quot;), and provides the output content.  ","version":"Next","tagName":"h2"},{"title":"Creating a Custom Tool​","type":1,"pageTitle":"Build Your Own Reasoning Application","url":"/xrx-core/docs/tutorials/bring_your_own_reasoning_agent#creating-a-custom-tool","content":" xRx allows you to create custom tools that extend your agent's capabilities. These tools enable your agent to perform specific tasks or access particular information sources, greatly enhancing its functionality. This feature is one of the most powerful aspects of the xRx framework.  Let's walk through the process of creating a custom tool, using a simple calculator as an example. This tool will allow your agent to perform basic arithmetic operations.  First, we'll create the tool function:  from agent_framework import observability_decorator @observability_decorator(name=&quot;calculator&quot;) def calculator(operation: str, x: float, y: float) -&gt; str: &quot;&quot;&quot; Perform a basic arithmetic operation. Args: operation (str): The operation to perform (add, subtract, multiply, divide) x (float): The first number y (float): The second number Returns: str: A string containing the result of the operation &quot;&quot;&quot; if operation == &quot;add&quot;: result = x + y elif operation == &quot;subtract&quot;: result = x - y elif operation == &quot;multiply&quot;: result = x * y elif operation == &quot;divide&quot;: if y != 0: result = x / y else: return &quot;Error: Division by zero&quot; else: return &quot;Error: Invalid operation&quot; return f&quot;The result of {x} {operation} {y} is {result}&quot;   This function takes an operation and two numbers as input, performs the specified operation, and returns the result as a string. The @observability_decorator is a special feature of xRx that allows for monitoring and logging of tool usage.  Now, to make this tool available to your agent, you need to add it to the tools_dict in the single_turn_agent function:  tools_dict = { &quot;get_current_weather&quot;: get_current_weather, &quot;get_current_time&quot;: get_current_time, &quot;get_stock_price&quot;: get_stock_price, &quot;calculator&quot;: calculator, # Add your new tool here }   By adding the calculator to this dictionary, you're telling your agent that it has access to this new capability. The language model can now use this tool when it determines that a calculation is necessary to answer a query.  This is just a simple example, but it illustrates the power and flexibility of the tool system in xRx. You could create tools to access databases, call external APIs, process images, or perform any other task that Python can handle. The key is to think about what specific capabilities would enhance your agent's ability to assist users in your particular domain.  Remember, when creating tools, it's important to:  Clearly define the tool's purpose and inputsHandle potential errors gracefullyReturn results in a format that's easy for the agent to interpret and useUse the @observability_decorator for monitoring and debugging  As you develop your agent, you'll likely find yourself creating a variety of custom tools to handle different tasks. This modular approach allows you to continually expand and refine your agent's capabilities over time.  ","version":"Next","tagName":"h2"},{"title":"Test your reasoning agent​","type":1,"pageTitle":"Build Your Own Reasoning Application","url":"/xrx-core/docs/tutorials/bring_your_own_reasoning_agent#test-your-reasoning-agent","content":" Once you've implemented your agent and added any custom tools, it's time to test it. You can do this by building and running a Docker container:  cd customer-support-agent docker build -t customer-support-agent:latest . docker run -p 8003:8003 --env-file .env customer-support-agent:latest   Your agent will now be accessible at http://localhost:8003.  ","version":"Next","tagName":"h2"},{"title":"Build a custom UI​","type":1,"pageTitle":"Build Your Own Reasoning Application","url":"/xrx-core/docs/tutorials/bring_your_own_reasoning_agent#build-a-custom-ui","content":" While the agent works fine via API calls, you might want to create a custom user interface. For information on how to build custom widgets for your UI, refer to the tutorial Create your own Widgets.  ","version":"Next","tagName":"h2"},{"title":"Create your .env file​","type":1,"pageTitle":"Build Your Own Reasoning Application","url":"/xrx-core/docs/tutorials/bring_your_own_reasoning_agent#create-your-env-file","content":" Your agent will need certain environment variables to function correctly. Create a .env file in your project directory based on the provided .env.example. Here's a minimal example:  LLM_API_KEY=&quot;&lt;your Api Key&gt;&quot; LLM_BASE_URL=&quot;https://api.groq.com/openai/v1&quot; LLM_MODEL_ID=&quot;llama3-70b-8192&quot;   Make sure to replace the placeholder values with your actual API keys and preferred model ID.  ","version":"Next","tagName":"h2"},{"title":"Deploy the xRx system​","type":1,"pageTitle":"Build Your Own Reasoning Application","url":"/xrx-core/docs/tutorials/bring_your_own_reasoning_agent#deploy-the-xrx-system","content":" With your agent implemented and environment variables set, you're ready to deploy the full xRx system. From your project directory, run:  docker-compose up --build   This command builds and starts all the necessary containers for your xRx application.  ","version":"Next","tagName":"h2"},{"title":"Project Structure​","type":1,"pageTitle":"Build Your Own Reasoning Application","url":"/xrx-core/docs/tutorials/bring_your_own_reasoning_agent#project-structure","content":" Here's a quick overview of the key components in your project:  app/: Contains the main application code agent/: Agent logic executor.py: Main agent execution logic tools/: Folder containing agent toolsmain.py: FastAPI application setup and endpoint definition test/: Contains test filesxrx-core/: Core xRx framework (submodule)Dockerfile: Docker configuration for containerizationrequirements.txt: Python dependenciesdocker-compose.yaml: Docker Compose configuration file  ","version":"Next","tagName":"h2"},{"title":"API Usage​","type":1,"pageTitle":"Build Your Own Reasoning Application","url":"/xrx-core/docs/tutorials/bring_your_own_reasoning_agent#api-usage","content":" Your agent exposes a single endpoint via FastAPI:  POST /run-reasoning-agent: Submit a query to the reasoning agent and receive streaming responses.  Here's an example of how to use this endpoint with curl:  curl -X POST http://localhost:8003/run-reasoning-agent \\ -H &quot;Content-Type: application/json&quot; \\ -H &quot;Accept: text/event-stream&quot; \\ -d '{ &quot;session&quot;: { &quot;id&quot;: &quot;1234567890&quot; }, &quot;messages&quot;: [ {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is the weather like in Paris?&quot;} ] }'   This setup provides a solid foundation for building and customizing your own reasoning agent within the xRx framework. As you continue to develop your agent, remember that the key to creating a powerful and effective AI assistant lies in thoughtfully designing its capabilities, carefully implementing its logic, and thoroughly testing its performance across a wide range of scenarios.  Happy building! ","version":"Next","tagName":"h2"},{"title":"Changing TTS providers","type":0,"sectionRef":"#","url":"/xrx-core/docs/tutorials/changing_tts_providers","content":"","keywords":"","version":"Next"},{"title":"Supported TTS Providers​","type":1,"pageTitle":"Changing TTS providers","url":"/xrx-core/docs/tutorials/changing_tts_providers#supported-tts-providers","content":" Currently, xRx supports the following TTS providers:  ElevenlabsDeepgramOpenAI  ","version":"Next","tagName":"h2"},{"title":"Configuring the TTS Provider​","type":1,"pageTitle":"Changing TTS providers","url":"/xrx-core/docs/tutorials/changing_tts_providers#configuring-the-tts-provider","content":" To change the TTS provider, you need to modify the environment variables in your .env file. Here's how to do it:  Open your .env file located at the root of the xRx repository. Locate the TTS-related environment variables:  # === Text-to-speech options === # TTS provider. Choices are &quot;elevenlabs&quot;, &quot;deepgram&quot;, &quot;openai&quot;, or &quot;cartesia&quot; TTS_PROVIDER=&quot;elevenlabs&quot; TTS_SAMPLE_RATE=&quot;24000&quot; # Elevenlabs ELEVENLABS_API_KEY=&quot;&lt;Elevenlabs API key&gt;&quot; ELEVENLABS_VOICE_ID=&quot;&lt;Elevenlabs Voice ID&gt;&quot; ... # Deepgram DG_API_KEY=&quot;&lt;Deepgram API key&gt;&quot; DG_TTS_MODEL_VOICE=&quot;aura-asteria-en&quot; # OpenAI OPENAI_API_KEY=&quot;&lt;OpenAI API key&gt;&quot; OPENAI_TTS_MODEL=&quot;&lt;OpenAI TTS model&gt;&quot; OPENAI_TTS_VOICE=&quot;&lt;OpenAI TTS voice&gt;&quot;   Change the TTS_PROVIDER value to your desired provider: For Elevenlabs: TTS_PROVIDER=&quot;elevenlabs&quot;For Deepgram: TTS_PROVIDER=&quot;deepgram&quot;For OpenAI: TTS_PROVIDER=&quot;openai&quot; Provide the necessary API keys and configuration for your chosen provider.  ","version":"Next","tagName":"h2"},{"title":"Provider-Specific Configuration​","type":1,"pageTitle":"Changing TTS providers","url":"/xrx-core/docs/tutorials/changing_tts_providers#provider-specific-configuration","content":" ","version":"Next","tagName":"h2"},{"title":"Elevenlabs​","type":1,"pageTitle":"Changing TTS providers","url":"/xrx-core/docs/tutorials/changing_tts_providers#elevenlabs","content":" For Elevenlabs, you need to set the following environment variables:  ELEVENLABS_API_KEY=&quot;&lt;your Elevenlabs API key&gt;&quot; ELEVENLABS_VOICE_ID=&quot;&lt;your chosen Voice ID&gt;&quot; ELEVENLABS_MODEL_ID=&quot;eleven_turbo_v2.5&quot; ELEVENLABS_VOICE_STABILITY=&quot;0.9&quot; ELEVENLABS_VOICE_SIMILARITY=&quot;0.9&quot;   You can sign up for an Elevenlabs account and obtain an API key at https://elevenlabs.io/app/sign-up.  ","version":"Next","tagName":"h3"},{"title":"Deepgram​","type":1,"pageTitle":"Changing TTS providers","url":"/xrx-core/docs/tutorials/changing_tts_providers#deepgram","content":" For Deepgram, you need to set the following environment variables:  DG_API_KEY=&quot;&lt;your Deepgram API key&gt;&quot; DG_TTS_MODEL_VOICE=&quot;aura-asteria-en&quot;   You can sign up for a Deepgram account and obtain an API key at https://deepgram.com/.  ","version":"Next","tagName":"h3"},{"title":"OpenAI​","type":1,"pageTitle":"Changing TTS providers","url":"/xrx-core/docs/tutorials/changing_tts_providers#openai","content":" For OpenAI, you need to set the following environment variables:  OPENAI_API_KEY=&quot;&lt;your OpenAI API key&gt;&quot; OPENAI_TTS_MODEL=&quot;&lt;your chosen TTS model&gt;&quot; OPENAI_TTS_VOICE=&quot;&lt;your chosen TTS voice&gt;&quot;   You can sign up for an OpenAI account and obtain an API key at https://platform.openai.com/signup.  ","version":"Next","tagName":"h3"},{"title":"Applying Changes​","type":1,"pageTitle":"Changing TTS providers","url":"/xrx-core/docs/tutorials/changing_tts_providers#applying-changes","content":" After modifying the .env file:  Save the changes to your .env file.Restart your xRx system for the changes to take effect:  docker-compose down docker-compose up --build   ","version":"Next","tagName":"h2"},{"title":"Troubleshooting​","type":1,"pageTitle":"Changing TTS providers","url":"/xrx-core/docs/tutorials/changing_tts_providers#troubleshooting","content":" If you encounter issues after changing the TTS provider:  Double-check your .env file to ensure all required variables are set correctly.Verify that you have the necessary API keys and they are valid.Check the logs of the TTS service for any error messages:  docker-compose logs xrx-tts   Ensure that your chosen TTS provider is properly configured and accessible from your deployment environment.  ","version":"Next","tagName":"h2"},{"title":"Technical Notes​","type":1,"pageTitle":"Changing TTS providers","url":"/xrx-core/docs/tutorials/changing_tts_providers#technical-notes","content":" The default sample rate for all models is 24000Hz. If a model uses a different sample rate, the .env variable TTS_SAMPLE_RATE should be changed to match the sample rate of the model.There is a limit of 4000 characters for the maximum input length sent to the TTS API. This can be adjusted if the TTS API has smaller limits.The TTS service implements a caching mechanism to improve performance. Synthesized audio is cached on disk in the cache directory.Every TTS model is implemented with streaming capabilities for fast responses.  By following these steps, you can easily switch between different TTS providers in your xRx system, allowing you to choose the best option for your specific use case and requirements. ","version":"Next","tagName":"h2"},{"title":"Text-To-Speech (TTS)","type":0,"sectionRef":"#","url":"/xrx-core/docs/references/tts","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Text-To-Speech (TTS)","url":"/xrx-core/docs/references/tts#overview","content":" The Text-to-Speech (TTS) module is a FastAPI service that provides real-time synthesis of text to speech using multiple TTS providers. This module establishes a WebSocket connection to receive text input and returns synthesized speech in binary format.  ","version":"Next","tagName":"h2"},{"title":"Endpoint​","type":1,"pageTitle":"Text-To-Speech (TTS)","url":"/xrx-core/docs/references/tts#endpoint","content":" URL: /api/v1/wsMethod: GETProtocol: WebSocket  ","version":"Next","tagName":"h3"},{"title":"Authentication​","type":1,"pageTitle":"Text-To-Speech (TTS)","url":"/xrx-core/docs/references/tts#authentication","content":" Header: None required for the WebSocket connection itself.Environment Variables: TTS_PROVIDER: Specifies the TTS provider to use (default: &quot;elevenlabs&quot;)Provider-specific API keys and settings (see Environment Variables section)  ","version":"Next","tagName":"h3"},{"title":"Connection​","type":1,"pageTitle":"Text-To-Speech (TTS)","url":"/xrx-core/docs/references/tts#connection","content":" Upon connecting to the WebSocket endpoint, the server initializes the specified TTS provider and prepares to handle incoming messages.  ","version":"Next","tagName":"h3"},{"title":"Messages​","type":1,"pageTitle":"Text-To-Speech (TTS)","url":"/xrx-core/docs/references/tts#messages","content":" The server handles JSON messages to initiate or cancel the text-to-speech synthesis process.  JSON Messages​  Format: JSONFields: action: &quot;synthesize&quot; or &quot;cancel&quot;text: The text content to be synthesized (required if action is &quot;synthesize&quot;)  Example:  { &quot;action&quot;: &quot;synthesize&quot;, &quot;text&quot;: &quot;Hello, how can I assist you today?&quot; }   ","version":"Next","tagName":"h3"},{"title":"Events​","type":1,"pageTitle":"Text-To-Speech (TTS)","url":"/xrx-core/docs/references/tts#events","content":" message​  Description: Triggered when a message is received from the client.Parameters: message: The message data (JSON)  Handling Synthesize Messages:  If action is &quot;synthesize&quot;, the server will begin the synthesis process and stream the resulting audio data back to the client.  Handling Cancel Messages:  If action is &quot;cancel&quot;, the server will cancel any ongoing synthesis tasks.  close​  Description: Triggered when the client closes the connection.  ","version":"Next","tagName":"h3"},{"title":"Server Responses​","type":1,"pageTitle":"Text-To-Speech (TTS)","url":"/xrx-core/docs/references/tts#server-responses","content":" Binary Responses​  Format: Binary data (audio data)Description: The synthesized speech audio data is sent in chunks as it is received from the TTS provider.  JSON Responses​  Format: JSONFields: action: Indicates completion with &quot;done&quot;  Example:  { &quot;action&quot;: &quot;done&quot; }   ","version":"Next","tagName":"h3"},{"title":"Error Handling​","type":1,"pageTitle":"Text-To-Speech (TTS)","url":"/xrx-core/docs/references/tts#error-handling","content":" If the TTS provider connection fails or an error occurs during synthesis, the server logs the error and closes the WebSocket connection.The server caches synthesized audio to avoid redundant processing for identical text inputs.  ","version":"Next","tagName":"h3"},{"title":"Environment Variables​","type":1,"pageTitle":"Text-To-Speech (TTS)","url":"/xrx-core/docs/references/tts#environment-variables","content":" TTS_PROVIDER: Specifies the TTS provider to use (default: &quot;elevenlabs&quot;)TTS_SAMPLE_RATE: Specifies the sample rate for the TTS output (default: 24000Hz)ElevenLabs: ELEVENLABS_API_KEY: API key for ElevenLabs authenticationELEVENLABS_VOICE_ID: Voice ID for the ElevenLabs TTS serviceELEVENLABS_MODEL_ID: Model ID for ElevenLabs (default: &quot;eleven_turbo_v2.5&quot;)ELEVENLABS_VOICE_STABILITY: Voice stability setting (default: 0.9)ELEVENLABS_VOICE_SIMILARITY: Voice similarity setting (default: 0.9) OpenAI: OPENAI_API_KEY: API key for OpenAI authenticationOPENAI_TTS_MODEL: OpenAI TTS model to use (default: &quot;tts-1&quot;)OPENAI_TTS_VOICE: OpenAI TTS voice to use (default: &quot;alloy&quot;) Deepgram: DG_API_KEY: API key for Deepgram authenticationDG_TTS_MODEL_VOICE: Deepgram TTS model and voice (default: &quot;aura-asteria-en&quot;) Cartesia: CARTESIA_API_KEY: API key for Cartesia authenticationCARTESIA_VOICE_ID: Voice ID for the Cartesia TTS serviceCARTESIA_MODEL_ID: Model ID for Cartesia (default: &quot;sonic-english&quot;)CARTESIA_VERSION: API version for Cartesia (default: &quot;2024-06-10&quot;) CACHE_DIR: Directory for caching audio data (default: &quot;cache&quot;)  ","version":"Next","tagName":"h3"},{"title":"Supported TTS Providers​","type":1,"pageTitle":"Text-To-Speech (TTS)","url":"/xrx-core/docs/references/tts#supported-tts-providers","content":" ElevenLabsOpenAIDeepgramCartesia  ","version":"Next","tagName":"h3"},{"title":"Technical Details​","type":1,"pageTitle":"Text-To-Speech (TTS)","url":"/xrx-core/docs/references/tts#technical-details","content":" ","version":"Next","tagName":"h2"},{"title":"TTS Interface​","type":1,"pageTitle":"Text-To-Speech (TTS)","url":"/xrx-core/docs/references/tts#tts-interface","content":" The TTS service uses an abstract base class TTSInterface that defines the common interface for all TTS providers. Each provider implements this interface with the following abstract methods:  class TTSInterface(ABC): @abstractmethod async def initialize(self): &quot;&quot;&quot;Initialize the TTS service.&quot;&quot;&quot; pass @abstractmethod async def synthesize(self, text: str): &quot;&quot;&quot;Synthesize text to audio and yield audio chunks.&quot;&quot;&quot; pass @abstractmethod async def close(self): &quot;&quot;&quot;Close the connection to the service.&quot;&quot;&quot; pass @property @abstractmethod def is_open(self) -&gt; bool: &quot;&quot;&quot;Check if the connection is open.&quot;&quot;&quot; pass   ","version":"Next","tagName":"h3"},{"title":"Example Usage​","type":1,"pageTitle":"Text-To-Speech (TTS)","url":"/xrx-core/docs/references/tts#example-usage","content":" Connecting to the WebSocket:  const socket = new WebSocket('ws://localhost:8002/api/v1/ws'); socket.onopen = () =&gt; { console.log('Connection opened'); }; socket.onmessage = (event) =&gt; { if (typeof event.data === 'string') { const message = JSON.parse(event.data); if (message.action === 'done') { console.log('Synthesis completed'); } } else { console.log('Received binary audio data'); // Handle audio data (e.g., play it or save it) } }; socket.onclose = (event) =&gt; { console.log('Connection closed'); }; socket.onerror = (error) =&gt; { console.error('WebSocket error:', error); };   Sending a Synthesize Message:  const message = { action: 'synthesize', text: 'Hello, how can I assist you today?' }; socket.send(JSON.stringify(message));   Sending a Cancel Message:  const message = { action: 'cancel' }; socket.send(JSON.stringify(message));   This documentation provides an overview of the Text-to-Speech WebSocket API, including connection details, message formats, events, supported providers, and example usage. The API supports multiple TTS providers and includes configuration options through environment variables. ","version":"Next","tagName":"h2"},{"title":"Enabling LLM Guardrails","type":0,"sectionRef":"#","url":"/xrx-core/docs/tutorials/enabling_llm_guardrails","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Enabling LLM Guardrails","url":"/xrx-core/docs/tutorials/enabling_llm_guardrails#overview","content":" The xRx system uses a Guardrails Proxy to implement safety measures for input and output processing. This proxy acts as an intermediary between the orchestrator and the reasoning agent, applying various checks such as toxic language detection.  ","version":"Next","tagName":"h2"},{"title":"Setup​","type":1,"pageTitle":"Enabling LLM Guardrails","url":"/xrx-core/docs/tutorials/enabling_llm_guardrails#setup","content":" To enable LLM guardrails, you need to configure the orchestrator to use the Guardrails Proxy instead of connecting directly to the reasoning agent. Here's how to do it:  Open the .env file in the root directory of your project. If you don't have a .env file, you will need to create one in the root directory of your project with the necessary environment variables. Here is an example of an .env file with the required variables set. Update or add the following environment variables: AGENT_HOST=&quot;xrx-guardrails&quot; AGENT_PORT=&quot;8094&quot; GUARDRAILS_AGENT_HOST=&quot;xrx-reasoning&quot; GUARDRAILS_AGENT_PORT=&quot;8003&quot; This configuration tells the orchestrator to connect to the Guardrails Proxy (xrx-guardrails) instead of the reasoning agent directly. It also specifies the host and port for the actual reasoning agent (xrx-reasoning) that the Guardrails Proxy will communicate with. Save the .env file.  To apply these changes, rebuild and restart your containers:  docker compose up --build   ","version":"Next","tagName":"h2"},{"title":"Advanced Configuration​","type":1,"pageTitle":"Enabling LLM Guardrails","url":"/xrx-core/docs/tutorials/enabling_llm_guardrails#advanced-configuration","content":" ","version":"Next","tagName":"h2"},{"title":"Monitoring specific fields​","type":1,"pageTitle":"Enabling LLM Guardrails","url":"/xrx-core/docs/tutorials/enabling_llm_guardrails#monitoring-specific-fields","content":" The Guardrails Proxy can be configured to monitor specific endpoints and apply different checks. To customize the guardrails:  Open the guardrails-proxy/app/main.py file. Locate the MONITOR_CONFIG dictionary. This defines which endpoints are monitored and how to extract content for validation. Modify or add entries to suit your API structure. For example: MONITOR_CONFIG = { &quot;my-custom-endpoint&quot;: { &quot;input_keys&quot;: { &quot;messages&quot;: { &quot;type&quot;: &quot;list&quot;, &quot;list_index&quot;: -1, &quot;items&quot;: { &quot;type&quot;: &quot;object&quot;, &quot;field&quot;: &quot;content&quot;, } } } } } This configuration tells the proxy to monitor the input body of &quot;my-custom-endpoint&quot; endpoint, extract content from the last message in the input request, and apply guardrails to the &quot;content&quot; field of the response.  ","version":"Next","tagName":"h3"},{"title":"Customized guardrails​","type":1,"pageTitle":"Enabling LLM Guardrails","url":"/xrx-core/docs/tutorials/enabling_llm_guardrails#customized-guardrails","content":" For more advanced guardrails or custom checks, you can:  To add or modify guardrails, edit the guardrails-proxy/app/guard.py file. You can chain multiple guardrails using the Guard().use_many() method.Implement additional guardrail models from the Guardrails AI Hub.Create custom guardrail classes in the guard.py file.Adjust the proxy's behavior in main.py to handle different types of requests or responses.  By following these steps, you can effectively enable and customize LLM guardrails in your xRx system, enhancing the safety and reliability of your AI interactions. ","version":"Next","tagName":"h3"},{"title":"Speech-To-Text (STT)","type":0,"sectionRef":"#","url":"/xrx-core/docs/references/stt","content":"","keywords":"","version":"Next"},{"title":"Overview​","type":1,"pageTitle":"Speech-To-Text (STT)","url":"/xrx-core/docs/references/stt#overview","content":" This WebSocket API allows clients to connect to a server for real-time speech-to-text (STT) transcription. The server supports multiple STT providers and handles both text and binary messages.  ","version":"Next","tagName":"h3"},{"title":"Endpoint​","type":1,"pageTitle":"Speech-To-Text (STT)","url":"/xrx-core/docs/references/stt#endpoint","content":" URL: /api/v1/wsMethod: GETProtocol: WebSocket  ","version":"Next","tagName":"h3"},{"title":"Environment Variables​","type":1,"pageTitle":"Speech-To-Text (STT)","url":"/xrx-core/docs/references/stt#environment-variables","content":" STT_PROVIDER: Specifies the STT provider to use (&quot;faster_whisper&quot;, &quot;groq&quot;, &quot;deepgram&quot;). Default is &quot;faster_whisper&quot;.  ","version":"Next","tagName":"h3"},{"title":"Connection​","type":1,"pageTitle":"Speech-To-Text (STT)","url":"/xrx-core/docs/references/stt#connection","content":" Upon connecting to the WebSocket endpoint, the server will log the connection and initialize the STT model.  ","version":"Next","tagName":"h3"},{"title":"Messages​","type":1,"pageTitle":"Speech-To-Text (STT)","url":"/xrx-core/docs/references/stt#messages","content":" The server handles binary messages containing audio data.  Binary Messages​  Format: Binary audio data. All STT providers are expecting PCM, 16-bit, 16000 Hz, mono audio.  ","version":"Next","tagName":"h3"},{"title":"Events​","type":1,"pageTitle":"Speech-To-Text (STT)","url":"/xrx-core/docs/references/stt#events","content":" message​  Description: Triggered when a binary message (audio data) is received from the client.Parameters: data: The binary message data (audio)  Handling Binary Messages:  The server will transcribe the binary data (audio) to text using the configured STT model and send the transcribed text back to the client.  close​  Description: Triggered when the client closes the connection.Parameters: code: The close codereason: The reason for closing  ","version":"Next","tagName":"h3"},{"title":"Server Responses​","type":1,"pageTitle":"Speech-To-Text (STT)","url":"/xrx-core/docs/references/stt#server-responses","content":" Text Responses​  Format: Plain textContent: The transcribed text from the audio data  ","version":"Next","tagName":"h3"},{"title":"Error Handling​","type":1,"pageTitle":"Speech-To-Text (STT)","url":"/xrx-core/docs/references/stt#error-handling","content":" If the STT provider is unsupported, the server raises a ValueError.If the WebSocket connection is disconnected, the server logs the disconnection and closes the STT model.  ","version":"Next","tagName":"h3"},{"title":"Example Usage​","type":1,"pageTitle":"Speech-To-Text (STT)","url":"/xrx-core/docs/references/stt#example-usage","content":" Connecting to the WebSocket:  const socket = new WebSocket('ws://localhost:8000/api/v1/ws'); socket.onopen = () =&gt; { console.log('Connection opened'); }; socket.onmessage = (event) =&gt; { console.log('Received message:', event.data); }; socket.onclose = (event) =&gt; { console.log(`Connection closed: ${event.code} ${event.reason}`); }; socket.onerror = (error) =&gt; { console.error('WebSocket error:', error); };   Sending a Binary Message:  const audioData = new Uint8Array([/* audio data */]); socket.send(audioData);   ","version":"Next","tagName":"h3"},{"title":"FastAPI Application Code​","type":1,"pageTitle":"Speech-To-Text (STT)","url":"/xrx-core/docs/references/stt#fastapi-application-code","content":" from fastapi import FastAPI, WebSocket, WebSocketDisconnect import os import logging from groq_stt import GroqSTT from deepgram_stt import DeepGramSTT from faster_whisper_stt import FasterWhisperSTT app = FastAPI() STT_PROVIDER = os.environ.get(&quot;STT_PROVIDER&quot;, &quot;faster_whisper&quot;) # Configure logging logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) class STTFactory: _instance = None @classmethod def get_instance(cls, provider: str): if cls._instance is None: if provider == &quot;faster_whisper&quot;: cls._instance = FasterWhisperSTT() elif provider == &quot;groq&quot;: cls._instance = GroqSTT() elif provider == &quot;deepgram&quot;: cls._instance = DeepGramSTT() else: raise ValueError(f&quot;Unsupported STT provider: {provider}&quot;) return cls._instance @app.websocket(&quot;/api/v1/ws&quot;) async def websocket_endpoint(websocket: WebSocket): stt_model = STTFactory.get_instance(STT_PROVIDER) await websocket.accept() try: logger.info(&quot;WebSocket connection established.&quot;) async def text_handler(text): await websocket.send_text(text) await stt_model.initialize(text_handler) while True: data = await websocket.receive_bytes() result = await stt_model.transcribe(data) if result: await websocket.send_text(result) except WebSocketDisconnect: await stt_model.close() logger.info(&quot;Client disconnected&quot;)   ","version":"Next","tagName":"h3"},{"title":"STT Interface Code​","type":1,"pageTitle":"Speech-To-Text (STT)","url":"/xrx-core/docs/references/stt#stt-interface-code","content":" from abc import ABC, abstractmethod class STTInterface(ABC): &quot;&quot;&quot;Interface for speech-to-text services.&quot;&quot;&quot; @abstractmethod async def initialize(self, text_handler: callable = None): &quot;&quot;&quot;Initialize the STT service.&quot;&quot;&quot; pass @abstractmethod async def transcribe(self, data: bytearray) -&gt; str: &quot;&quot;&quot;Transcribe audio data to text.&quot;&quot;&quot; pass @abstractmethod async def close(self): &quot;&quot;&quot;Close the connection to the service.&quot;&quot;&quot; pass @property @abstractmethod def is_open(self) -&gt; bool: &quot;&quot;&quot;Check if the connection is open.&quot;&quot;&quot; pass   This documentation provides an overview of the WebSocket API, including connection details, message formats, events, and example usage for the STT service. ","version":"Next","tagName":"h3"},{"title":"Run the Shopify Application","type":0,"sectionRef":"#","url":"/xrx-core/docs/tutorials/run_shopify_applications","content":"","keywords":"","version":"Next"},{"title":"Store Configuration​","type":1,"pageTitle":"Run the Shopify Application","url":"/xrx-core/docs/tutorials/run_shopify_applications#store-configuration","content":" To get started with the quick service restaurant agent, you need to create a Shopify shop.  ","version":"Next","tagName":"h2"},{"title":"Sign Up for Shopify and Create a Shop​","type":1,"pageTitle":"Run the Shopify Application","url":"/xrx-core/docs/tutorials/run_shopify_applications#sign-up-for-shopify-and-create-a-shop","content":" Go to the Shopify website and sign up for a new account by clicking Start free trial. Follow the on-screen instructions to set up your shop. You can learn more on the Shopify getting started page. For this tutorial, a blank shop is all you will need to continue.  ","version":"Next","tagName":"h3"},{"title":"Import Products​","type":1,"pageTitle":"Run the Shopify Application","url":"/xrx-core/docs/tutorials/run_shopify_applications#import-products","content":" As soon as you create the store, Shopify will ask you to import some products. You can make your own products, or you can use the products we have provided in the config/shopify/pizza-store.csv file here.  If you skipped the onboarding and you have no products in the store, do the following: to import the products to your shop, navigate to the admin link of your Shopify Shop (https://admin.shopify.com/store/SHOP_NAME) and click on Products in the left-hand menu. Then click on Import and upload the pizza-store.csv file from your computer.  ","version":"Next","tagName":"h3"},{"title":"Generate API Credentials​","type":1,"pageTitle":"Run the Shopify Application","url":"/xrx-core/docs/tutorials/run_shopify_applications#generate-api-credentials","content":" In this section, we will generate four specific Shopify variables which are needed for the xRx Shopify agent: SHOPIFY_API_KEY, SHOPIFY_TOKEN, SHOPIFY_SHOP, and SHOPIFY_SHOP_GID.  Start by configuring the API scopes for your shop  Navigate to the Shopify admin panel. This will be a url like https://admin.shopify.com/store/{your_shop_name}.Go to Settings and click on Apps and sales channels.Click on Develop apps.Click on Allow custom app development, and again click on Allow custom app development.Click on Create an app.Enter a name for your app, such as xRx App and click on Create app.On the next page, click on Configure Admin API scopes.Select the Read and write option and click on Continue.In the example solution, the minimum scopes are read_products, write_draft_orders, read_draft_orders, write_orders, and read_orders.Scroll down and click on Save.  Next you need to generate your API key, token, and shop name (SHOPIFY_API_KEY, SHOPIFY_TOKEN, SHOPIFY_SHOP).  Go to the API credentials tab.Scroll down to API key and secret key, and note down the &quot;API key&quot; This is the SHOPIFY_API_KEY.Click on Install app to generate an access token.Click on Reveal token once and note down the SHOPIFY_TOKEN. It should start with shpat_. WARNING: YOU WILL ONLY BE ABLE TO SEE THE TOKEN ONCE, SO MAKE SURE TO WRITE IT DOWN! On the upper left of the screen, you should see the url for the shop. For example, abc123-def.myshopify.com. Note down the first part of the url, for example, abc123-def. This is the SHOPIFY_SHOP.  Lastly, you need your shop's global identifier (SHOPIFY_SHOP_GID). This is a bit harder to find.  Option 1: use the Shopify admin api. Here is a curl command which you can run to get the shop's global ID. Note that the SHOPIFY_TOKEN is the same as the one you generated above and the SHOPIFY_SHOP is the same as the one you noted down. This command will produce a JSON object which contains a string that says &quot;gid://shopify/Shop/1234&quot;. The 1234 is the global ID.  curl -X POST \\ -H &quot;X-Shopify-Access-Token: $SHOPIFY_TOKEN&quot; \\ -H &quot;Content-Type: application/json&quot; \\ -d '{&quot;query&quot;: &quot;{ shop { id } }&quot;}' \\ &quot;https://$SHOPIFY_SHOP.myshopify.com/admin/api/2023-07/graphql.json&quot;   Option 2: go to the shop admin page, create an order, go to the order page and the url in the address bar will be in the format of https://shopify.com/123456789/account/orders/ABCD5678. The 123456789 is the global ID SHOPIFY_SHOP_GID.  Important Check: if you do not have your SHOPIFY_API_KEY, SHOPIFY_TOKEN, SHOPIFY_SHOP, and SHOPIFY_SHOP_GID ready, please return to the top of this section and continue from there.  ","version":"Next","tagName":"h3"},{"title":"Set up the .env file​","type":1,"pageTitle":"Run the Shopify Application","url":"/xrx-core/docs/tutorials/run_shopify_applications#set-up-the-env-file","content":" If you have already configured xRx with a .env file, skip this step. If you are new to xRx, you just need to activate and update the .env example present in the Shopify agent application folder.  Update the .env file with the following:  Update the SHOPIFY_API_KEY, SHOPIFY_TOKEN, SHOPIFY_SHOP, and SHOPIFY_SHOP_GID environment variables with the values you noted down in the above steps.Update the additional environment variables. Documentation on this can be found in the Quick Start section.Update the NEXT_PUBLIC_UI variable with either &quot;pizza-agent&quot; or &quot;shoe-agent&quot; depending on your store.Update the SHOPIFY_STORE_INFO with the name of your shop. This is what the agent will refer to your shop as. We recommend, Shoe Shop based on the products we will be using in this example.Update the SHOPIFY_CUSTOMER_SERVICE_TASK with the task you want the agent to perform. For example: SHOPIFY_CUSTOMER_SERVICE_TASK=&quot;You are a customer service representative who is helping customers order items from the shop. You are courteous, helpful and concise.&quot;   ","version":"Next","tagName":"h3"},{"title":"Check Redis Integration​","type":1,"pageTitle":"Run the Shopify Application","url":"/xrx-core/docs/tutorials/run_shopify_applications#check-redis-integration","content":" No action is needed for this section if you are using the docker-compose setup and pre-provided .env file.  The quick service restaurant agent uses a Redis container (xrx-redis) to shop and manage task statuses. This allows for efficient, real-time status updates and checks across the distributed system.  If you are using the docker-compose setup, the Redis container will be automatically started and the reasoning agent will be able to use it as long as the environment variable is correctly set as shown below.  REDIS_HOST=&quot;xrx-redis&quot;   If you are running the agent locally outside of docker compose, the reasoning agent will look for a Redis container on the default host (localhost) and port (6379). In order to start that server, you can use the following command:  docker run -d --name redis-server -p 6379:6379 redis   ","version":"Next","tagName":"h2"},{"title":"Deploy the Containers​","type":1,"pageTitle":"Run the Shopify Application","url":"/xrx-core/docs/tutorials/run_shopify_applications#deploy-the-containers","content":" Once you have completed the above steps, you can deploy the containers by running the following command:  docker compose up --build   Enjoy experimenting! ","version":"Next","tagName":"h2"},{"title":"Run the Patient Intake Application","type":0,"sectionRef":"#","url":"/xrx-core/docs/tutorials/run_patient_intake_application","content":"","keywords":"","version":"Next"},{"title":"Check Redis Integration​","type":1,"pageTitle":"Run the Patient Intake Application","url":"/xrx-core/docs/tutorials/run_patient_intake_application#check-redis-integration","content":" No action is needed for this section if you are using the docker-compose setup and pre-provided .env file.  The Shopify agent uses a Redis container (xrx-redis) to shop and manage task statuses. This allows for efficient, real-time status updates and checks across the distributed system.  If you are using the docker-compose setup, the Redis container will be automatically started and the reasoning agent will be able to use it as long as the environment variable is correctly set as shown below.  REDIS_HOST=&quot;xrx-redis&quot;   If you are running the agent locally outside of docker compose, the reasoning agent will look for a Redis container on the default host (localhost) and port (6379). In order to start that server, you can use the following command:  docker run -d --name redis-server -p 6379:6379 redis   ","version":"Next","tagName":"h2"},{"title":"Deploy the Containers​","type":1,"pageTitle":"Run the Patient Intake Application","url":"/xrx-core/docs/tutorials/run_patient_intake_application#deploy-the-containers","content":" Once you have completed the above step, you can deploy the containers by running the following command:  docker compose up --build   This use case is super simple and ready to be deployed. Enjoy experimenting! ","version":"Next","tagName":"h2"},{"title":"Using the UI Debug Mode","type":0,"sectionRef":"#","url":"/xrx-core/docs/tutorials/using_the_ui_debug_mode","content":"Using the UI Debug Mode When you are using xRx, it is sometimes helpful to view the conversational interaction as text instead of purely audio. This tutorial will guide you through setting up and using the built in text debugging feature of the Next.js front end in xRx. WarningThis feature is only available when you run the client with npm run dev and not via docker compose because the docker compose file doesn't pass it to the build context. The base experience of xRx looks like this: In order to see the text transcript of everything you have said to your reasoning agent, you need to define the NEXT_PUBLIC_UI_DEBUG_MODE environment variable in your .env file. This file should be located at the root of your project. # .env NEXT_PUBLIC_UI_DEBUG_MODE=true You can then run the project with the normal docker compose command: docker compose up --build Setting this variable to true will enable the ability to toggle between voice and text modes. A new button will appear on the UI like below: Once you click the button, the UI will switch to text mode where you can see the existing conversational history. Happy debugging!","keywords":"","version":"Next"},{"title":"Creating your own Widgets","type":0,"sectionRef":"#","url":"/xrx-core/docs/tutorials/create_your_own_widgets","content":"","keywords":"","version":"Next"},{"title":"Information Flow Details​","type":1,"pageTitle":"Creating your own Widgets","url":"/xrx-core/docs/tutorials/create_your_own_widgets#information-flow-details","content":" ","version":"Next","tagName":"h2"},{"title":"Agent Generation​","type":1,"pageTitle":"Creating your own Widgets","url":"/xrx-core/docs/tutorials/create_your_own_widgets#agent-generation","content":" The agent generates a widget output in the Widget class located in the shopify-app/reasoning/app/agent/graph/nodes/widget.py file. This method matches the tool used by the agent to a specific widget type and creates a widget output dictionary in the following format:  widget_output = { 'type': 'widget-type-name', 'details': json.dumps(tool_output), 'available-tools': [ { 'tool': 'tool-name', 'arguments': [ 'arg1', 'arg2', ], }, ] }   The 'tool-name' in the 'available-tools' array corresponds to user actions that can be taken on the widget output. These tools represent the next possible actions a user can perform based on the current widget data. These actions are represented as more tools. For example, in a product_list tool might spawn a widget which has an available-tools array which contains a view_product_details tool which the user can click on to view more information about the product.  ","version":"Next","tagName":"h3"},{"title":"Executor Processing​","type":1,"pageTitle":"Creating your own Widgets","url":"/xrx-core/docs/tutorials/create_your_own_widgets#executor-processing","content":" The run_agent function in shopify-app/reasoning/app/agent/executor.py processes the widget output. It formats the widget data and includes it in the response stream in the following format:  { &quot;messages&quot;: &quot;&lt;messages from agent&gt;&quot;, &quot;session&quot;: &quot;&lt;session for shopify and front end&gt;&quot;, &quot;node&quot;: &quot;Widget&quot;, &quot;output&quot;: &quot;&lt;widget output from step 1&gt;&quot;, &quot;reason&quot;: &quot;&lt;reason for widget output&gt;&quot; }   It is important to note that this &quot;node&quot;: &quot;Widget&quot; key value pair is extremely important to the orchestrator and front-end to know that the output is a widget which must be rendered.  ","version":"Next","tagName":"h3"},{"title":"Orchestrator Handling​","type":1,"pageTitle":"Creating your own Widgets","url":"/xrx-core/docs/tutorials/create_your_own_widgets#orchestrator-handling","content":" The orchestrator (orchestrator/src/Session.ts) receives the widget data and passes it to the front-end:  if(type === 'Widget') { this.server.log.debug(`Received from Agent:${agentResponse}`); this.server.log.debug('Not sending to TTS'); this.textResponse('agent', agentResponse, 'widget'); }   agentResponse is the response from the &quot;output&quot; key from executor which includes the widget data in JSON format.  ","version":"Next","tagName":"h3"},{"title":"Front-end Rendering​","type":1,"pageTitle":"Creating your own Widgets","url":"/xrx-core/docs/tutorials/create_your_own_widgets#front-end-rendering","content":" The front-end (shopify-app/nextjs-client/src/app/page.tsx) receives the widget data and renders it via a switch statement. The renderWidget function processes the widget data and returns the appropriate JSX based on the widget type. For example:  switch (widget.type) { case 'shopify-product-list': return ( &lt;div className=&quot;widget&quot; id=&quot;product-list&quot;&gt; &lt;h2&gt;Products&lt;/h2&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;&lt;/th&gt; &lt;th&gt;Title&lt;/th&gt; &lt;th&gt;Options&lt;/th&gt; &lt;th&gt;Action&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; {Object.entries(details).map(([productId, product]: [string, any]) =&gt; ( &lt;tr key={productId}&gt; &lt;td&gt;&lt;img src={`https://placehold.co/100x100?text=${encodeURIComponent(product.product_title)}`} alt={product.product_title} /&gt;&lt;/td&gt; &lt;td&gt;{product.product_title}&lt;/td&gt; &lt;td&gt;{product.options.map((opt: any) =&gt; opt.option_title).join(', ')}&lt;/td&gt; &lt;td&gt; &lt;div className=&quot;button-container&quot;&gt; {loadingButtons[`details-${productId}`] ? ( &lt;HashLoader size={20} color={&quot;#F15950&quot;} /&gt; ) : ( &lt;button className=&quot;widget-button&quot; onClick={(event) =&gt; showDetails(event.target, parseInt(productId))}&gt;View Details&lt;/button&gt; )} &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt; ))} &lt;/tbody&gt; &lt;/table&gt; &lt;/div&gt; );   The details variable above maps to the agentResponse variable from the orchestrator.  ","version":"Next","tagName":"h3"},{"title":"Creating Custom Widgets​","type":1,"pageTitle":"Creating your own Widgets","url":"/xrx-core/docs/tutorials/create_your_own_widgets#creating-custom-widgets","content":" To create your own custom widgets, you'll need to modify code in several places:  Agent Widget Generation: In reasoning/shopify-agent/app/agent/graph/nodes/widget.py, add a new case to the match_widget_to_tool function: elif tool == 'your_new_tool': widget_output = { 'type': 'your-new-widget-type', 'details': json.dumps(tool_output), 'available-tools': [ { 'tool': 'next_possible_tool', 'arguments': ['arg1', 'arg2'], }, ] } Executor Processing: Ensure that the run_agent function in shopify-app/reasoning/app/agent/executor.py can handle your new widget type. The existing code should work for new widgets as long as they follow the standard format. Front-end Widget Rendering: In shopify-app/nextjs-client/src/app/page.tsx, add a new case to the renderWidget function: case 'your-new-widget-type': return ( &lt;div className=&quot;widget&quot; id=&quot;your-new-widget&quot;&gt; &lt;h2&gt;Your New Widget&lt;/h2&gt; &lt;table&gt; &lt;thead&gt; &lt;tr&gt; {/* Add your table headers */} &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; {Object.entries(details).map(([itemId, item]: [string, any]) =&gt; ( &lt;tr key={itemId}&gt; {/* Add your table cells */} &lt;td&gt; &lt;div className=&quot;button-container&quot;&gt; {loadingButtons[`action-${itemId}`] ? ( &lt;HashLoader size={20} color={&quot;#F15950&quot;} /&gt; ) : ( &lt;button className=&quot;widget-button&quot; onClick={(event) =&gt; handleAction(event.target, itemId)}&gt;Action&lt;/button&gt; )} &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt; ))} &lt;/tbody&gt; &lt;/table&gt; &lt;/div&gt; ); Add New Tool (if necessary): If your widget requires a new tool, add it to shopify-app/reasoning/app/agent/tools/shopify.py: @observability_decorator(name=&quot;your_new_tool&quot;) def your_new_tool(arg1: str, arg2: int) -&gt; dict: &quot;&quot;&quot; Docstring describing your new tool... &quot;&quot;&quot; try: # Your tool logic here return result_dict except Exception as e: raise e Update Agent Logic (if necessary): If your widget requires new agent logic, update the relevant files in the shopify-app/reasoning/app/agent/ directory.  ","version":"Next","tagName":"h2"},{"title":"Using a Custom Reasoning Agent​","type":1,"pageTitle":"Creating your own Widgets","url":"/xrx-core/docs/tutorials/create_your_own_widgets#using-a-custom-reasoning-agent","content":" When using a custom reasoning agent, you can still leverage the existing widget system as long as your agent's output follows the JSON format provided by the executor. This format is crucial for the orchestrator and front-end to properly handle and render the widgets.  To ensure compatibility:  Your custom agent should output widget data in the following format:  { &quot;messages&quot;: &quot;&lt;messages from agent&gt;&quot;, &quot;session&quot;: &quot;&lt;session for shopify and front end&gt;&quot;, &quot;node&quot;: &quot;Widget&quot;, &quot;output&quot;: { &quot;type&quot;: &quot;your-widget-type&quot;, &quot;details&quot;: &quot;&lt;JSON string of widget details&gt;&quot;, &quot;available-tools&quot;: [ { &quot;tool&quot;: &quot;tool-name&quot;, &quot;arguments&quot;: [&quot;arg1&quot;, &quot;arg2&quot;] } ] }, &quot;reason&quot;: &quot;&lt;reason for widget output&gt;&quot; }   Ensure that the &quot;node&quot; key is set to &quot;Widget&quot; for any widget output. This is crucial for the orchestrator to recognize the output as a widget. The &quot;output&quot; object should contain the widget type, details, and available tools, following the structure shown above. The &quot;tool-name&quot; in the &quot;available-tools&quot; array corresponds to user actions that can be taken on the widget output. These tools represent the next possible actions a user can perform based on the current widget data. For example: In a product list widget, a tool named &quot;view_product_details&quot; might correspond to a &quot;View Details&quot; button for each product.In an order summary widget, a tool named &quot;confirm_order&quot; could map to a &quot;Confirm Order&quot; button. When implementing your custom widget in the front-end, you'll need to create appropriate UI elements (like buttons) that trigger these tool actions when clicked.  As long as your custom agent adheres to this format, the existing orchestrator and front-end components will be able to process and render the widgets without requiring additional modifications. This allows for flexibility in creating custom reasoning agents while still leveraging the established widget rendering system.  The mapping between tool names and user actions provides a powerful way to create interactive widgets that can trigger further actions in your agent, allowing for a dynamic and responsive user experience. ","version":"Next","tagName":"h2"},{"title":"Enabling LLM Observability","type":0,"sectionRef":"#","url":"/xrx-core/docs/tutorials/setting_up_llm_observability","content":"","keywords":"","version":"Next"},{"title":"Infrastructure Setup​","type":1,"pageTitle":"Enabling LLM Observability","url":"/xrx-core/docs/tutorials/setting_up_llm_observability#infrastructure-setup","content":" ","version":"Next","tagName":"h2"},{"title":"Langfuse (self hosted)​","type":1,"pageTitle":"Enabling LLM Observability","url":"/xrx-core/docs/tutorials/setting_up_llm_observability#langfuse-self-hosted","content":" IMPORTANT: this is the only completely free way we support observability at the moment  Clone the Langfuse repo here  Because the front end client runs on port 3000 for xRx, you must change the Langfuse port forwarding to a port other than 3000 in your local deployment via docker for Langfuse. You can do this yourself by altering their docker-compose.yml. Or, we have provide a docker-compose.yml here that you can use to start a Langfuse server on a different port (3001).  Run docker compose up to start the Langfuse server  Go to http://localhost:3001 to access the Langfuse dashboard. Create an account and create a new project. Then create an API key for that project.  In the .env file, set the following environment variables:  LLM_OBSERVABILITY_LIBRARY=&quot;langfuse&quot; LANGFUSE_SECRET_KEY=&quot;&lt;your Langfuse Secret key&gt;&quot; LANGFUSE_PUBLIC_KEY=&quot;&lt;your Langfuse Public key&gt;&quot; LANGFUSE_HOST=&quot;http://host.docker.internal:3001&quot;   Tracing will then be sent to your own Langfuse instance hosted on your local machine.  ","version":"Next","tagName":"h3"},{"title":"Langfuse (cloud hosted via Langfuse)​","type":1,"pageTitle":"Enabling LLM Observability","url":"/xrx-core/docs/tutorials/setting_up_llm_observability#langfuse-cloud-hosted-via-langfuse","content":" Ensure you have an account created here  In the .env file, set the following environment variables:  LLM_OBSERVABILITY_LIBRARY=&quot;langfuse&quot; LANGFUSE_SECRET_KEY=&quot;&lt;your Langfuse Secret key&gt;&quot; LANGFUSE_PUBLIC_KEY=&quot;&lt;your Langfuse Public key&gt;&quot; LANGFUSE_HOST=&quot;https://us.cloud.langfuse.com&quot;   Tracing will automatically then go to the Langfuse cloud hosted service.  ","version":"Next","tagName":"h3"},{"title":"LangSmith​","type":1,"pageTitle":"Enabling LLM Observability","url":"/xrx-core/docs/tutorials/setting_up_llm_observability#langsmith","content":" Ensure you have an account created here  In the .env file, set the following environment variables:  LLM_OBSERVABILITY_LIBRARY=&quot;langsmith&quot; LANGCHAIN_TRACING_V2=true LANGCHAIN_API_KEY=&quot;&lt;your Langsmith API key&gt;&quot; LANGCHAIN_PROJECT= &quot;&lt;your Langsmith project name&gt;&quot; LANGCHAIN_ENDPOINT=&quot;https://api.smith.langchain.com&quot;   Traces will be automatically sent to LangSmith.  ","version":"Next","tagName":"h3"},{"title":"Disabling Observability (not recommended)​","type":1,"pageTitle":"Enabling LLM Observability","url":"/xrx-core/docs/tutorials/setting_up_llm_observability#disabling-observability-not-recommended","content":" If you would like to disable observability, you can do so by setting the following environment variable:  LLM_OBSERVABILITY_LIBRARY=&quot;none&quot;   ","version":"Next","tagName":"h3"},{"title":"Example .env file​","type":1,"pageTitle":"Enabling LLM Observability","url":"/xrx-core/docs/tutorials/setting_up_llm_observability#example-env-file","content":" An example .env file which has been enabled for using a local Langfuse instance can be found here.  # === LLM options === LLM_API_KEY=&quot;&lt;your Api Key&gt;&quot; LLM_BASE_URL=&quot;https://api.groq.com/openai/v1&quot; LLM_MODEL_ID=&quot;llama3-70b-8192&quot; # === Agent options === INITIAL_RESPONSE=&quot;Hello! How can I help you?&quot; # === LLM observability options === # note: make the host domain &quot;localhost&quot; if you are running outside of docker compose LANGFUSE_HOST_DOMAIN=&quot;host.docker.internal&quot; LANGFUSE_SECRET_KEY=&quot;&lt;your Langfuse secret key&gt;&quot; LANGFUSE_PUBLIC_KEY=&quot;&lt;your Langfuse public key&gt;&quot; LANGFUSE_HOST=&quot;http://${LANGFUSE_HOST_DOMAIN}:3001&quot; # === Text-to-speech options === TTS_PROVIDER=&quot;elevenlabs&quot; ELEVENLABS_API_KEY=&quot;&lt;your Elevenlabs xi_api_key&gt;&quot; ELEVENLABS_VOICE_ID=&quot;&lt;your Elevenlabs voice_id&gt;&quot; # === Speech-to-text options === # STT provider. Choices are &quot;groq, &quot;deepgram&quot;, or &quot;faster_whisper&quot; STT_PROVIDER=&quot;deepgram&quot; # Deepgram DG_API_KEY=&quot;&lt;Deepgram API key&gt;&quot;   ","version":"Next","tagName":"h3"},{"title":"Alter Reasoning Code​","type":1,"pageTitle":"Enabling LLM Observability","url":"/xrx-core/docs/tutorials/setting_up_llm_observability#alter-reasoning-code","content":" ","version":"Next","tagName":"h2"},{"title":"LLM Clients​","type":1,"pageTitle":"Enabling LLM Observability","url":"/xrx-core/docs/tutorials/setting_up_llm_observability#llm-clients","content":" xRx reasoning agents are designed to be observable by default. To ensure observability, follow these steps:  Use an OpenAI API Compatible Endpoint: Ensure that any LLM used in the reasoning agent is accessed via an OpenAI API compatible endpoint. Initialize LLM Clients with Tracing: Utilize the provided utility function to enable tracing. This function is part of the initialize_llm_client() method in the reasoning agent. You can find the code for this function in this file. Ensure Tracing for All LLM Calls: By initializing all LLM clients with the initialize_llm_client() function, tracing will be enabled for all LLM calls.  Here is an example of how to use this function:  from agent.utils.llm import initialize_llm_client llm_client = initialize_llm_client() ... response = llm_client.chat.completions.create( model=llm_model_id, messages=messages, temperature=0.9, )   Note: Based on if you need an async or sync LLM client, you would need to modify the initialize_llm_client() function to get the appropriate OpenAI client.  ","version":"Next","tagName":"h3"},{"title":"Other tracing elements​","type":1,"pageTitle":"Enabling LLM Observability","url":"/xrx-core/docs/tutorials/setting_up_llm_observability#other-tracing-elements","content":" For the components of a reasoning system that do not directly call an LLM (e.g., tool calls, orchestration, session operations, etc.), you can use the observability_decorator function decorator to ensure that tracing is enabled for these functions. The code for this decorator can be found in this file. This decorator is compatible with LangSmith, Langfuse, and &quot;none&quot; (no observability).  We highly recommend always using this decorator, even if you are not currently implementing observability. This will allow you to &quot;switch on&quot; observability at any point in the future.  Here is an example of how to use this decorator:  from agent.utils.llm import observability_decorator @observability_decorator(name=&quot;higher_than_2&quot;) def higher_than_2(x: int): return x &gt; 2  ","version":"Next","tagName":"h3"}],"options":{"languages":["en"],"indexDocs":true,"indexBlog":true,"indexPages":false,"hashed":true,"id":"default"}}