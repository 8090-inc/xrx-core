"use strict";(self.webpackChunkdocusaurus=self.webpackChunkdocusaurus||[]).push([[192],{8593:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>d,frontMatter:()=>s,metadata:()=>r,toc:()=>c});var i=t(4848),o=t(8453);const s={sidebar_position:5},l="Enabling LLM Observability",r={id:"tutorials/setting_up_llm_observability",title:"Enabling LLM Observability",description:"Through our own experimentation and development, we have found that observability is a critical component of debugging effective reasoning systems. It is highly recommended that you take advantage of this feature of the xRx repo to save yourself debugging time.",source:"@site/content/tutorials/setting_up_llm_observability.md",sourceDirName:"tutorials",slug:"/tutorials/setting_up_llm_observability",permalink:"/xrx-core/docs/tutorials/setting_up_llm_observability",draft:!1,unlisted:!1,editUrl:"https://github.com/8090-inc/xrx-core/blob/develop/docs/content/tutorials/setting_up_llm_observability.md",tags:[],version:"current",sidebarPosition:5,frontMatter:{sidebar_position:5},sidebar:"tutorialSidebar",previous:{title:"Enabling LLM Guardrails",permalink:"/xrx-core/docs/tutorials/enabling_llm_guardrails"},next:{title:"Changing STT providers",permalink:"/xrx-core/docs/tutorials/changing_stt_providers"}},a={},c=[{value:"Infrastructure Setup",id:"infrastructure-setup",level:2},{value:"Langfuse (self hosted)",id:"langfuse-self-hosted",level:3},{value:"Langfuse (cloud hosted via Langfuse)",id:"langfuse-cloud-hosted-via-langfuse",level:3},{value:"LangSmith",id:"langsmith",level:3},{value:"Disabling Observability (not recommended)",id:"disabling-observability-not-recommended",level:3},{value:"Example .env file",id:"example-env-file",level:3},{value:"Alter Reasoning Code",id:"alter-reasoning-code",level:2},{value:"LLM Clients",id:"llm-clients",level:3},{value:"Other tracing elements",id:"other-tracing-elements",level:3}];function h(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"enabling-llm-observability",children:"Enabling LLM Observability"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Through our own experimentation and development, we have found that observability is a critical component of debugging effective reasoning systems. It is highly recommended that you take advantage of this feature of the xRx repo to save yourself debugging time."})}),"\n",(0,i.jsx)(n.p,{children:"Currently, the xRx repository supports Langfuse (cloud service and self hosted) and LangSmith as frameworks for LLM observability."}),"\n",(0,i.jsx)(n.h2,{id:"infrastructure-setup",children:"Infrastructure Setup"}),"\n",(0,i.jsx)(n.h3,{id:"langfuse-self-hosted",children:"Langfuse (self hosted)"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"IMPORTANT: this is the only completely free way we support observability at the moment"})}),"\n",(0,i.jsxs)(n.p,{children:["Clone the Langfuse repo ",(0,i.jsx)(n.a,{href:"https://github.com/langfuse/langfuse",children:"here"})]}),"\n",(0,i.jsxs)(n.p,{children:["Because the front end client runs on port 3000 for xRx, you must change the Langfuse port forwarding to a port other than 3000 in your local deployment via docker for Langfuse. You can do this yourself by altering their ",(0,i.jsx)(n.a,{href:"https://github.com/langfuse/langfuse/blob/main/docker-compose.yml",children:"docker-compose.yml"}),". Or, we have provide a  ",(0,i.jsx)(n.code,{children:"docker-compose.yml"})," ",(0,i.jsx)(n.a,{href:"https://github.com/8090-inc/xrx/blob/main/llm-observability/langfuse/docker-compose.yml",children:"here"})," that you can use to start a Langfuse server on a different port (3001)."]}),"\n",(0,i.jsxs)(n.p,{children:["Run ",(0,i.jsx)(n.code,{children:"docker compose up"})," to start the Langfuse server"]}),"\n",(0,i.jsxs)(n.p,{children:["Go to ",(0,i.jsx)(n.a,{href:"http://localhost:3001",children:"http://localhost:3001"})," to access the Langfuse dashboard. Create an account and create a new project. Then create an API key for that project."]}),"\n",(0,i.jsxs)(n.p,{children:["In the ",(0,i.jsx)(n.code,{children:".env"})," file, set the following environment variables:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'LLM_OBSERVABILITY_LIBRARY="langfuse"\nLANGFUSE_SECRET_KEY="<your Langfuse Secret key>"\nLANGFUSE_PUBLIC_KEY="<your Langfuse Public key>"\nLANGFUSE_HOST="http://host.docker.internal:3001"\n'})}),"\n",(0,i.jsx)(n.p,{children:"Tracing will then be sent to your own Langfuse instance hosted on your local machine."}),"\n",(0,i.jsx)(n.h3,{id:"langfuse-cloud-hosted-via-langfuse",children:"Langfuse (cloud hosted via Langfuse)"}),"\n",(0,i.jsxs)(n.p,{children:["Ensure you have an account created ",(0,i.jsx)(n.a,{href:"https://us.cloud.langfuse.com",children:"here"})]}),"\n",(0,i.jsxs)(n.p,{children:["In the ",(0,i.jsx)(n.code,{children:".env"})," file, set the following environment variables:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'LLM_OBSERVABILITY_LIBRARY="langfuse"\nLANGFUSE_SECRET_KEY="<your Langfuse Secret key>"\nLANGFUSE_PUBLIC_KEY="<your Langfuse Public key>"\nLANGFUSE_HOST="https://us.cloud.langfuse.com"\n'})}),"\n",(0,i.jsx)(n.p,{children:"Tracing will automatically then go to the Langfuse cloud hosted service."}),"\n",(0,i.jsx)(n.h3,{id:"langsmith",children:"LangSmith"}),"\n",(0,i.jsxs)(n.p,{children:["Ensure you have an account created ",(0,i.jsx)(n.a,{href:"https://smith.langchain.com",children:"here"})]}),"\n",(0,i.jsxs)(n.p,{children:["In the ",(0,i.jsx)(n.code,{children:".env"})," file, set the following environment variables:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'LLM_OBSERVABILITY_LIBRARY="langsmith"\nLANGCHAIN_TRACING_V2=true\nLANGCHAIN_API_KEY="<your Langsmith API key>"\nLANGCHAIN_PROJECT= "<your Langsmith project name>"\nLANGCHAIN_ENDPOINT="https://api.smith.langchain.com"\n'})}),"\n",(0,i.jsx)(n.p,{children:"Traces will be automatically sent to LangSmith."}),"\n",(0,i.jsx)(n.h3,{id:"disabling-observability-not-recommended",children:"Disabling Observability (not recommended)"}),"\n",(0,i.jsx)(n.p,{children:"If you would like to disable observability, you can do so by setting the following environment variable:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'LLM_OBSERVABILITY_LIBRARY="none"\n'})}),"\n",(0,i.jsx)(n.h3,{id:"example-env-file",children:"Example .env file"}),"\n",(0,i.jsxs)(n.p,{children:["An example .env file which has been enabled for using a local Langfuse instance can be found ",(0,i.jsx)(n.a,{href:"https://github.com/8090-inc/xrx/blob/main/config/env-examples/env.langfuse",children:"here"}),"."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'# === LLM options ===\nLLM_API_KEY="<your Api Key>"\nLLM_BASE_URL="https://api.groq.com/openai/v1"\nLLM_MODEL_ID="llama3-70b-8192"\n\n# === Agent options ===\nINITIAL_RESPONSE="Hello! How can I help you?"\n\n# === LLM observability options ===\n# note: make the host domain "localhost" if you are running outside of docker compose\nLANGFUSE_HOST_DOMAIN="host.docker.internal"\nLANGFUSE_SECRET_KEY="<your Langfuse secret key>"\nLANGFUSE_PUBLIC_KEY="<your Langfuse public key>"\nLANGFUSE_HOST="http://${LANGFUSE_HOST_DOMAIN}:3001"\n\n# === Text-to-speech options ===\nTTS_PROVIDER="elevenlabs"\nELEVENLABS_API_KEY="<your Elevenlabs xi_api_key>"\nELEVENLABS_VOICE_ID="<your Elevenlabs voice_id>"\n\n# === Speech-to-text options ===\n# STT provider. Choices are "groq, "deepgram", or "faster_whisper"\nSTT_PROVIDER="deepgram"\n\n# Deepgram\nDG_API_KEY="<Deepgram API key>"\n'})}),"\n",(0,i.jsx)(n.h2,{id:"alter-reasoning-code",children:"Alter Reasoning Code"}),"\n",(0,i.jsx)(n.h3,{id:"llm-clients",children:"LLM Clients"}),"\n",(0,i.jsx)(n.p,{children:"xRx reasoning agents are designed to be observable by default. To ensure observability, follow these steps:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Use an OpenAI API Compatible Endpoint"}),": Ensure that any LLM used in the reasoning agent is accessed via an OpenAI API compatible endpoint."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Initialize LLM Clients with Tracing"}),": Utilize the provided utility function to enable tracing. This function is part of the ",(0,i.jsx)(n.code,{children:"initialize_llm_client()"})," method in the reasoning agent."]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["You can find the code for this function in ",(0,i.jsx)(n.a,{href:"https://github.com/8090-inc/xrx/blob/develop/reasoning/simple-agent/app/agent/utils/llm.py",children:"this file"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Ensure Tracing for All LLM Calls"}),": By initializing all LLM clients with the ",(0,i.jsx)(n.code,{children:"initialize_llm_client()"})," function, tracing will be enabled for all LLM calls."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Here is an example of how to use this function:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"from agent.utils.llm import initialize_llm_client\n\nllm_client = initialize_llm_client()\n...\nresponse = llm_client.chat.completions.create(\n    model=llm_model_id,\n    messages=messages,\n    temperature=0.9,\n)\n"})}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Note:"})," Based on if you need an async or sync LLM client, you would need to modify the ",(0,i.jsx)(n.code,{children:"initialize_llm_client()"})," function to get the appropriate OpenAI client."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"other-tracing-elements",children:"Other tracing elements"}),"\n",(0,i.jsxs)(n.p,{children:["For the components of a reasoning system that do not directly call an LLM (e.g., tool calls, orchestration, session operations, etc.), you can use the ",(0,i.jsx)(n.code,{children:"observability_decorator"})," function decorator to ensure that tracing is enabled for these functions. The code for this decorator can be found in ",(0,i.jsx)(n.a,{href:"https://github.com/8090-inc/xrx/blob/develop/reasoning/simple-agent/app/agent/utils/llm.py",children:"this file"}),'. This decorator is compatible with LangSmith, Langfuse, and "none" (no observability).']}),"\n",(0,i.jsx)(n.p,{children:'We highly recommend always using this decorator, even if you are not currently implementing observability. This will allow you to "switch on" observability at any point in the future.'}),"\n",(0,i.jsx)(n.p,{children:"Here is an example of how to use this decorator:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from agent.utils.llm import observability_decorator\n\n@observability_decorator(name="higher_than_2")\ndef higher_than_2(x: int):\n    return x > 2\n'})})]})}function d(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>l,x:()=>r});var i=t(6540);const o={},s=i.createContext(o);function l(e){const n=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:l(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);